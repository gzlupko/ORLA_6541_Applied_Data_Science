---
title: "Exercise 4"
author: "Nilima Ajaikumar, Hagay Jalon & Gian Zlupko"
date: "2022-11-14"
output: html_document
---

```{r message = FALSE, warning = FALSE}
library(tidyverse) 
library(xlsx) 
library(pROC) 
library(OptimalCutpoints) # for dealing with cutoff point
library(naniar) # replaces missing data to NA
```


# Part I: NECS Data 

For the first part of our Exercise 4, we are recreating the figures and tables published in Bowers & Zhou (2019). Our code and results for this section are organized as follows. First, we load and clean the NCES data. Then, we provide the code, documentation, and interpretation for Tables 1-3 and Figures 1-3 from Bowers & Zhou (2019) in the order in which they appear in the article. 

#### Import Data 

The following steps were implemented to access data from NCES' online repository. 


```{r}
setwd("/Users/gianzlupko/Desktop/ORLA 6541 Data Science/ORLA_6541_Applied_Data_Science/NCES_Data") 

# Load R Data File
load("els_full_data.rdata")

# simplify name of data frame 
els_full_data <- els_02_12_byf3pststu_v1_0


# variables that we would like to keep 

keepvars <- c(
   "STU_ID",
   "F2EVERDO", # Dropout
   "BYP52E", # Absent*
   "BYS24F", # Suspension*
   "BYP51", # Misbehavior*
   "BYTXMSTD", # Math t score (2002) - lowercase in the data
   "BYS42", # Extracurricular activities (2002) hours/wk
   "BYTXRSTD", # Reading t score (2002)* - lowercase in the data
   "F2PS0601", # College enrollment
   "F1RGPP2", # GPA
   "F1S27", # Extracurricular activities (2004) hours/wk
   "BYS33A", # AP
   "F3TZSTEM1CRED", # Postsecondary STEM degree
   "F3TZSTEM1TOT", # Number of STEM Courses
   "F3TZSTEM2GPA", # STEM course GPA
   "F3STEMOCCCUR" # Hard STEM career
)

# select subset of variables that were used in Bowers & Zhou (2019)
# also apply renaming to make the data readable 
els_clean <- els_full_data %>% 
  select(STU_ID, F2EVERDO, BYP52E, BYS24F, BYP51, bytxmstd, BYS42, bytxrstd,
         F2PS0601, F1RGPP2, F1S27, BYS33A, F3TZSTEM1CRED, F3TZSTEM1TOT,
         F3TZSTEM2GPA, F3STEMOCCCUR) %>%
  rename(student_ID = STU_ID, drop_out = F2EVERDO, absent = BYP52E, suspension = BYS24F, 
         misbehavior = BYP51, math_tscore = bytxmstd, extra_curr_2002 = BYS42, 
         reading_tscore = bytxrstd, college_enroll = F2PS0601, GPA = F1RGPP2, extra_curr_2004 = F1S27, AP = BYS33A, post_sec_stem = F3TZSTEM1CRED, no_stem_courses = F3TZSTEM1TOT, stem_GPA = F3TZSTEM2GPA, hard_stem_career = F3STEMOCCCUR)   

# inspect cleaning 
#els_clean


# finally, create the feature engineered flags that Bowers & Zhou also tested to look at 
# the impact of the potential implication of one or more 'flags' or common indicators of 
# student drop out 
# the flags include: absent, suspension math_tscore in the 1st percentile, and misbehavior 

els_vars <- els_clean %>%
  mutate(flag_00 = ifelse(rowSums(select(., c(3:6)), na.rm = T) >= 1, 1, 0), 
         flag_01 = ifelse(absent == 1 | suspension == 1 | math_tscore == 1 | misbehavior == 1, 1,0), flag_04 = ifelse(absent == 1 & suspension == 1 & math_tscore == 1 & misbehavior == 1, 1, 0))

```

Remove missing data in the data set that are stored as -9, -8, -4, -1

```{r}
library(naniar)
els_complete <- replace_with_na_if(els_vars,
                                   .predicate = is.numeric,
                                   condition = ~.x < 0)

# Create the math t-score variable following Bowers' CART markdown 

els_complete <- els_complete %>% 
  mutate(math_t_score = ifelse(math_tscore < 44.065, 1, 0), 
         reading_t_score = ifelse(reading_tscore < 43.64, 1, 0))


# finally, view summary of clean variables 
summary(els_complete) 
```


Following Bowers & Zhou (2019), the first table that we create below is Figure 1, which presents a confusion table that specifies common metrics used in machine learning to evaluate prediction performance. 


### Table 1: Confusion Table for Calculating Contingency Proportions 

```{r}

fig_1_stats <- c("Accuracy", "Precision", "Sensitivity (Recall / True-positive Proportion",
                  "Specificity (True-negative Proportion", "1-Specificity (False-positive Proportion", "Kapp")

fig_1_equations <- c("(a + d) / N", 
                     "a / (a + b)", 
                     "a / (a + c)", 
                     "d / (b + d)", 
                     "b / (b + d)", 
                     "(Accuracy - R) / (1 - R)")

# bind columns 
fig_1 <- data.frame(cbind(fig_1_stats, fig_1_equations)) 
colnames(fig_1) <- c("Metric", "Equation")
fig_1

```




### Figures 2A and 2B 

Figure A depicts a ROC plot that utilizes predictors used by Balfanz et al.  (2007) and Figure B uses predictors selected by Bowers & Zhou (2019). 


Figure 2A
```{r, message = FALSE}

# ROC for math t-score
roc1 = roc(els_complete$drop_out, els_complete$math_t_score, legacy.axes=TRUE, asp=FALSE, plot=TRUE, grid=FALSE, lty=1, xaxs="i", yaxs="i", main = "ROC curves for predicting dropout")

# ROC for one flag 
roc2 = roc(els_complete$drop_out, els_complete$flag_01, plot=TRUE, add=TRUE, percent=roc1$percent, lty=2)

# ROC for any number of flags 
roc3 = roc(els_complete$drop_out, els_complete$flag_00, plot=TRUE, add=TRUE, percent=roc1$percent, lty=3)

# ROC for all four
roc4 = roc(els_complete$drop_out, els_complete$flag_04, plot=TRUE, add=TRUE, percent=roc1$percent, lty=4)

# ROC for suspension 
roc5 = roc(els_complete$drop_out, els_complete$suspension, plot=TRUE, add=TRUE, percent=roc1$percent, lty=5)

# ROC for misbehavior
roc6 = roc(els_complete$drop_out, els_complete$misbehavior, plot=TRUE, add=TRUE, percent=roc1$percent, lty=6)

# ROC for absent
roc7 = roc(els_complete$drop_out, els_complete$absent, plot=TRUE, add=TRUE, percent=roc1$percent, lty=7)

# legend for the plot 
legend("bottomright", legend=c("Math t score (2002)","Any No. Flags","All Four Flags","Suspension", "Misbehavior", "Absent"), lty=c(1,2,3,4,5,6,7), lwd=2)

```


Figure 2B: 
```{r, message = FALSE}

# ROC for math t-score
roc8 = roc(els_complete$drop_out, els_complete$math_t_score, legacy.axes=TRUE, asp=FALSE, plot=TRUE, grid=FALSE, lty=7, xaxs="i", yaxs="i", main = "ROC curves for predicting dropout")

# ROC for reading t score   
roc9 = roc(els_complete$drop_out, els_complete$reading_t_score, plot=TRUE, add=TRUE, percent=roc1$percent, lty=8)

# ROC for absent 
roc10 = roc(els_complete$drop_out, els_complete$absent, plot=TRUE, add=TRUE, percent=roc1$percent, lty=9)

# ROC for suspension 
roc11 = roc(els_complete$drop_out, els_complete$absent, plot=TRUE, add=TRUE, percent=roc1$percent, lty=10)

# ROC for suspension 
roc12 = roc(els_complete$drop_out, els_complete$extra_curr_2002, plot=TRUE, add=TRUE, percent=roc1$percent, lty=11)


# legend for the plot 
legend("bottomright", legend=c("Math t score (2002)", "Reading t score (2002)", "Extrac. activities (2002)", "Absence", "Suspension"), lty=c(7,8,9,10,11), lwd=2)
  
  
```

Figure 2A and 2B depict ROC curves for key variables used to predict high school drop out. All of the predictors in Figure 2B predicted drop out better than chance. This result can be seen by visually inspecting each ROC curve and their distance from the diagonal line in the center of the plot, which indicates a random guess of 0.5. In contrast to Figure 2B, some of the predictors visualized in Figure 2A did not perform better than chance at predicting drop out. In particular, results in Figure 2A demonstrate that any number of flags performed worse than a random guess and all four flags performed about equivalent to a random guess in predictive capacity. 



##### AUC Values Corresponding with Figure 2A and 2B 

Below, we create the data table that is pictured below Figures 2A and 2B in Bowers & Zhou (2019) to show the AUC values that correspond with their respective ROC curves. 

Table under 2A: 
```{r}
# print AUC values and store to a vector
auc_fig2a <- c(roc1$auc, roc2$auc, roc3$auc, roc4$auc, roc5$auc, roc6$auc, roc7$auc) 

# store predictor names in a vector
predictor_fig2a <- c("Math t score (2002)", "Any one flag", "Any number of flags", "All four flags", "Suspension", 
                     "Misbehavior", "Absent")  

# combine vector into a data frame, rename columns, adjust decimal points, and sort desceding by top AUC

fig2a_table <- data.frame(cbind(predictor_fig2a, auc_fig2a)) 
colnames(fig2a_table) <- c("Predictor", "AUC") 

# roundabout convert from factor, have to go through character format first
fig2a_table$AUC <- as.numeric(as.character(fig2a_table$AUC)) 

fig2a_table %>% 
   dplyr::mutate_if(is.numeric, round, 2) 

```

Table under 2B: 
```{r}

# print AUC values and store to a vector
auc_fig2b <- c(roc8$auc, roc9$auc, roc10$auc, roc11$auc, roc12$auc) 

# store predictor names in a vector
predictor_fig2b <- c("Math t score (2002)", "Reading t score (2002)", "Exac. activities (2002)", 
                     "Absence", "Suspension") 

# combine vector into a data frame, rename columns, adjust decimal points, and sort desceding by top AUC

fig2b_table <- data.frame(cbind(predictor_fig2b, auc_fig2b)) 
colnames(fig2b_table) <- c("Predictor", "AUC") 

# roundabout convert from factor, have to go through character format first
fig2b_table$AUC <- as.numeric(as.character(fig2b_table$AUC)) 

fig2b_table %>% 
   dplyr::mutate_if(is.numeric, round, 2) 
```


Results from tables corresponding with Figures 2A and 2B indicated that math t score (2002) was the best predictor of high school drop out. Other predictors that performed well in predicting drop out were absences and involvement in extracurricular activities. 


#### Table 1: Significance of AUC Difference for Predictors of Continuous Dropout 

In our recreation of Table 1 below, we present results from a Delong test of the difference of two correlated ROC curves. Large Z scores and small p-values (below the conventional p < .05) indicate that the area under the respective curves is significantly different. For our table, we compared each predictor to math t score (2002) which was the predictor with the best AUC value observed on the current data. 

```{r}

# custom function to extra Z scores and p-values 
roc_sig_diff <- function(x, y) { 
   
test1 <- roc.test(x,y) 
print(test1$p.value) 
print(test1$statistic) 
   
   }

# cycle through all comparisons needed to build the table: 
roc_sig_diff(roc1, roc2)  
roc_sig_diff(roc1, roc3)  
roc_sig_diff(roc1, roc4)  
roc_sig_diff(roc1, roc5)  
roc_sig_diff(roc1, roc6)  
roc_sig_diff(roc1, roc7)  

# create columns for table 

pred_list_1 <- c("Math t score (2002)", "Math t score (2002)", "Math t score (2002)",
                 "Math t score (2002)", "Math t score (2002)", "Math t score (2002)")  

pred_list_2 <- c("One Flag", "Any number of flags", "All four flags", "Suspension", "Misbehavior", 
                 "Absence")

# create z score column 

z_scores <- c(26.78, 23.85, 23.85, 8.55, 6.45, 0.62)

# create p-value column 
p_values <- c("<.001","<.001", "<.001", "<.001", "0.53", "0.61")


# combine all columns and rename columns 

table_1 <- data.frame(cbind(pred_list_1, pred_list_2, z_scores, p_values)) 
table_1 %>% 
   rename("Predictor 1 " = pred_list_1, "Predictor 2" = pred_list_2, "Z" = z_scores, "p_value" = p_values)
```



Results in Table 1 show that math t score (2002) performed significantly better at predicting dropout than one flag, any number of flags, all four flags, and suspension. In contrast, the results also demonstrated that math t score did not have a significantly different AUC than misbehavior and absence.


### Figure 3: ROC Curves for Predicting College Enrollment and Postsecondary STEM Degree 

Figure 3A
```{r, message = FALSE}

# filter data to only include two values for college enroll: 0 (No), and 1 (Yes)
fig_3_data <- els_complete %>% 
   filter(college_enroll == 1 | college_enroll == 0) 

# ROC curves for GPA, extracurriculars, and AP 5 
# 'roc_enroll'... get it?.... couldn't resist...

roc_enroll_1 = roc(fig_3_data$college_enroll, fig_3_data$GPA, legacy.axes=TRUE, asp=FALSE, plot=TRUE, grid=FALSE, lty=12, xaxs="i", yaxs="i", main = "ROC curve for predicting college enrollment - GPA")

roc_enroll_2 = roc(fig_3_data$college_enroll, fig_3_data$extra_curr_2004, legacy.axes=TRUE, asp=FALSE, plot=TRUE, grid=FALSE, lty=13, xaxs="i", yaxs="i", main = "ROC curve for predicting college enrollment - Extra Curriculars")


roc_enroll_3 = roc(fig_3_data$college_enroll, fig_3_data$AP, legacy.axes=TRUE, asp=FALSE, plot=TRUE, grid=FALSE, lty=14, xaxs="i", yaxs="i", main = "ROC curve for predicting college enrollment - AP")

```

The table below shows the AUC values for each of the ROC curves above: 

```{r}

# print AUC values and store to a vector
auc_fig3a <- c(roc_enroll_1$auc, roc_enroll_2$auc, roc_enroll_3$auc) 

# store predictor names in a vector
predictor_fig3a <- c("GPA", "Extra Curriculars", "AP")     

# combine vector into a data frame, rename columns, adjust decimal points, and sort descending by top AUC

fig3a_table <- data.frame(cbind(predictor_fig3a, auc_fig3a)) 
colnames(fig3a_table) <- c("Predictor", "AUC") 

# roundabout convert from factor, have to go through character format first
fig3a_table$AUC <- as.numeric(as.character(fig3a_table$AUC)) 
fig3a_table %>% 
   dplyr::mutate_if(is.numeric, round, 2) 

```

The AUC values reported in the table indicate that GPA was the best predictor of college enrollment, followed by students' involvement in extra curricular activities. Finally, AP performed only slightly better than chance at predicting student college enrollment. 




#### Figure 3B 

Post-secondary STEM degree 

```{r, message = FALSE}
fig_3b_data <- els_complete %>% 
   filter(post_sec_stem == 1 | post_sec_stem == 0) %>% 
   mutate(as.factor(post_sec_stem)) 

# build ROC curves for number of STEM courses, math t scores, and STEM course GPA 
stem_degree1 = roc(fig_3b_data$post_sec_stem, fig_3b_data$no_stem_courses, legacy.axes=TRUE, asp=FALSE, plot=TRUE, grid=FALSE, lty=1, xaxs="i", yaxs="i", main = "ROC curve for predicting STEM degree - Num. STEM Courses")

stem_degree2 = roc(fig_3b_data$post_sec_stem, fig_3b_data$math_tscore, legacy.axes=TRUE, asp=FALSE, plot=TRUE, grid=FALSE, lty=2, xaxs="i", yaxs="i", main = "ROC curve for predicting STEM degree - Math t score (2002)")


stem_degree3 = roc(fig_3b_data$post_sec_stem, fig_3b_data$stem_GPA, legacy.axes=TRUE, asp=FALSE, plot=TRUE, grid=FALSE, lty=3, xaxs="i", yaxs="i", main = "ROC curve for predicting STEM degree - STEM GPA")

```


Results for the ROC curves plotted in Figure 3B indicate that Number of STEM courses was the best predictor of whether or not students obtained a postsecondary STEM degree. Results also indicated that math t score predicted whether students obtained a STEM degree after high school. The table below shows the AUC values for each of the ROC curves above: 

```{r}
# print AUC values and store to a vector
auc_fig3b <- c(stem_degree1$auc, stem_degree2$auc, stem_degree3$auc) 

# store predictor names in a vector
predictor_fig3b <- c("Number of STEM Courses", "Math t score (2002)", "STEM GPA")    

# combine vector into a data frame, rename columns, adjust decimal points, and sort desceding by top AUC

fig3b_table <- data.frame(cbind(predictor_fig3b, auc_fig3b)) 
colnames(fig3b_table) <- c("Predictor", "AUC") 

# roundabout convert from factor, have to go through character format first
fig3b_table$AUC <- as.numeric(as.character(fig3b_table$AUC)) 
fig3b_table %>% 
   dplyr::mutate_if(is.numeric, round, 2) 

```


Results for AUC values reported in the table above indicate that the number of STEM courses students took was the best predictor of whether students received a postsecondary degree in a STEM field. 


#### Table 2: Significance of AUC Difference for College Enrollment Predictors and STEM Degree Predictors

The table below conducts difference tests between the predictors of college enrollment and STEM careers that were reported above to determine which of the predictors were significantly different from one another. The first code chunk reports results from Delong's test of AUC comparisons for the predictors of college enrollment. The second code chunk does the same for STEM degree. 


```{r}
# code below utilizes custom created function from above 
# that spits out results from Delong's test
roc_sig_diff(roc_enroll_1, roc_enroll_2) 
roc_sig_diff(roc_enroll_1, roc_enroll_3) 
roc_sig_diff(roc_enroll_2, roc_enroll_3) 

# create columns for table 
pred_list_1 <- c("GPA", "GPA", "Extra Curricular (2004)")   
pred_list_2 <- c("Extra Curricular (2004)", "AP", "AP")

# create z score column 
z_scores <- c(15.19, 29.85, 8.82)

# create p-value column 
p_values <- c("<.001", "<.001", "<.001")

# combine all columns and rename columns 

table_1 <- data.frame(cbind(pred_list_1, pred_list_2, z_scores, p_values)) 
table_1 %>% 
   rename("Predictor 1 " = pred_list_1, "Predictor 2" = pred_list_2, "Z" = z_scores, "p_value" = p_values)

```

Predictor comparisons for STEM degree. 

```{r}
roc_sig_diff(stem_degree1, stem_degree2) 
roc_sig_diff(stem_degree1, stem_degree3) 
roc_sig_diff(stem_degree2, stem_degree3) 

# create columns for table 
pred_list_1 <- c("Number of STEM Courses", "Number of STEM Courses", "Math t score (2002)")     
pred_list_2 <- c("Math t score (2002)", "GPA", "GPA")

# create z score column 
z_scores <- c(31.62,39.65,8.71)

# create p-value column 
p_values <- c("<.001", "<.01", "<.001")

# combine all columns and rename columns 

table_1 <- data.frame(cbind(pred_list_1, pred_list_2, z_scores, p_values)) 
table_1 %>% 
   rename("Predictor 1 " = pred_list_1, "Predictor 2" = pred_list_2, "Z" = z_scores, "p_value" = p_values)
```

These results indicate that all of the AUC values were significantly different from one another. The largest difference was between that of number of STEM courses taken and GPA. This finding supports trends visualized by the ROC curves in Figure 3B, where the plots clearly indicate that number of STEM courses taken significantly outperformed student GPA on the prediction task. 




# Part II

#### Chapter 7 - Bulut & Dejardines (2019) Excercises

7.2 Decision trees in R

In this chapter, we will predict whether students' science score in the PISA data set are above or below the mean scale score for science.

Load relevant packages

```{r package-setup, message = FALSE, warning = FALSE}

library("data.table")
library("dplyr")
library("ggplot2")

#decision_packages <- c("caret", "rpart", "rpart.plot", "randomForest", "modelr")
#install.packages(decision_packages)

library("caret")
library("rpart")
library("rpart.plot")
library("randomForest")
library("modelr")
```

### Retrieve the pisa dataset

Loading the pisa data set will take some time as it is very large. 

```{r, message = FALSE}
pisa <- fread("pisa2015.csv",na.strings = "")
# check data
#head(pisa)
```
### Recode variables

```{r}
# custom function used to recode 
bin.to.num <- function(x) {
  if (is.na(x)) NA
  else if (x =="Yes") 1L
  else if (x == "No") 0L
  
}
```

### Recode variables

```{r}
#Create and recode variables
pisa[, `:=` 
     (female = ifelse(ST004D01T == "Female", 1, 0),
       sex = ST004D01T,
       
       # At my house we have ...
       desk = sapply(ST011Q01TA, bin.to.num),
       own.room = sapply(ST011Q02TA, bin.to.num),
       quiet.study = sapply(ST011Q03TA, bin.to.num),
       computer = sapply(ST011Q04TA, bin.to.num),
       software = sapply(ST011Q05TA, bin.to.num),
       internet = sapply(ST011Q06TA, bin.to.num),
       lit = sapply(ST011Q07TA, bin.to.num),
       poetry = sapply(ST011Q08TA, bin.to.num),
       art = sapply(ST011Q09TA, bin.to.num),
       book.sch = sapply(ST011Q10TA, bin.to.num),
       tech.book = sapply(ST011Q11TA, bin.to.num),
       dict = sapply(ST011Q12TA, bin.to.num),
       art.book = sapply(ST011Q16NA, bin.to.num))]

pisa[, `:=`
     (math = rowMeans(pisa[, c(paste0("PV", 1:10, "MATH"))], na.rm = TRUE),reading = rowMeans(pisa[, c(paste0("PV", 1:10, "READ"))], na.rm = TRUE), science = rowMeans(pisa[, c(paste0("PV", 1:10, "SCIE"))], na.rm = TRUE))]


```

#### The average science score in PISA 2015 was 493 across all participating countries. Using this score as a cut-off value, we will first create a binary variable called science_perf where science_perf= High if a student’s science score is equal or larger than 493; otherwise science_perf= Low

```{r }

pisa <- pisa[, science_perf := as.factor(ifelse(science >= 493, "High", "Low"))]

```

#### Create new dataset, pisa_small, which  will subset the students from the United States and Canada and choose some variables (rather than the entire set of variables) 

```{r }
pisa_small <- subset(pisa, CNT %in% c("Canada", "United States"), 
                     select = c(science_perf, WEALTH, HEDRES, ENVAWARE, ICTRES, 
                                EPIST, HOMEPOS, ESCS, reading, math))
```


#### Split  dataset into a training dataset (70%) and a test dataset (30%). 

```{r}

# Set the seed before splitting the data
set.seed(442019)

# We need to remove missing cases
pisa_nm <- na.omit(pisa_small)

# Split the data into training and test
index <- createDataPartition(pisa_nm$science_perf, p = 0.7, list = FALSE)
train_dat <- pisa_nm[index, ]
test_dat  <- pisa_nm[-index, ]

nrow(train_dat)
nrow(test_dat)

```

#### Buid a decision tree model df_fit1 with no pruning (i.e., cp = 0) and no cross-validation as we have a test dataset already (i.e., xval = 0). 
#### Here the Gini index is used for the splitting.

```{r}

dt_fit1 <- rpart(formula = science_perf ~ .,
                 data = train_dat,
                 method = "class", 
                 control = rpart.control(minsplit = 20, 
                                         cp = 0, 
                                         xval = 0),
                parms = list(split = "gini"))
```


#### The estimated model is very likely to have too many nodes because we set cp = 0.

Due to having many nodes, first we will examine the results graphically, before we attempt to print the output. Although the `rpart` package can draw decision tree plots, they are very basic. Therefore, we will use the `rpart.plot` function from the `rpart.plot package` to draw a nicer decision tree plot. 

```{r}
rpart.plot(dt_fit1)
```
#### The above model  is NOT very interpretable so we need to prune the trees; otherwise the model yields a very complex model with many nodes – which is very likely to overfit the data. 

#### In the following model, we use cp = 0.005.

```{r}

dt_fit2 <- rpart(formula = science_perf ~ .,
                 data = train_dat,
                 method = "class", 
                 control = rpart.control(minsplit = 20, 
                                         cp = 0.005, 
                                         xval = 0),
                parms = list(split = "gini"))

rpart.plot(dt_fit2)
```
#### We could also estimate the same model with the Entropy as the split criterion, split = "information", and the results would be similar (not necessarily the tree itself, but its classification performance).

```{r}

dt_fit2 <- rpart(formula = science_perf ~ .,
                 data = train_dat,
                 method = "class", 
                 control = rpart.control(minsplit = 20, 
                                         cp = 0.005, 
                                         xval = 0),
                parms = list(split = "information"))

```

#### Now our model is less complex compared to the previous model. 

Adjust colors to make the trees even more distinct. Also, adjust which values should be shown in the nodes, using extra = 8.

```{r}
# model using entropy
rpart.plot(dt_fit2, extra = 8, box.palette = "RdBu", shadow.col = "gray")

```

#### An alternative way to prune the model is to use the `prune()` function from the rpart package.
In the following example, we will use our initial complex model dt_fit1 and prune it.

```{r}

dt_fit1_prune <- prune(dt_fit1, cp = 0.005)
rpart.plot(dt_fit1_prune, extra = 8, box.palette = "RdBu", shadow.col = "gray")

```

#### Print the output of our model using `printcp()`

```{r}
printcp(dt_fit2)

```

#### Use `summary()` to print out more detailed results with all splits

```{r}
summary(dt_fit2)
```
#### Similarly, `varImp()` from the caret package also gives us a similar output showing variable importance. 

```{r}
varImp(dt_fit2)
```

#### Both of these show the importance of the variables for our estimated decision tree model. 
The larger the values are, the more crucial they are for the model.In our example, math and reading seem to be highly important for the decision tree model, whereas ICTRES is the least important variable.The variables that were not very important for the model are those that were not included in the final model. These variables  possibly have very low correlations with our outcome variable, science_perf.

We can use `rpart.rules` function to print out the decision rules from the trees. By default, the output from this function shows the probability of the second class for each decision/split being made (i.e., the category “low” in our example) and what percent of the observations fall into this category


```{r}
rpart.rules(dt_fit2, cover = TRUE)

```

#### Need to check the classification accuracy of the estimated decision tree with the test data. 

Otherwise, it is hard to justify whether or not the estimated decision tree would work accurately for prediction.

#### Estimate the predicted classes (either high or low) from the test data by applying the estimated model.
First we obtain model predictions using `predict()` and then turn the results into a data frame called dt_pred.

```{r}

dt_pred <- predict(dt_fit2, test_dat) %>%
  as.data.frame()

head(dt_pred)

```

#### The above dataset shows each observation’s (i.e., students from the test data) probability of falling into either high or low categories based on the decision rules that we estimated. 
We will turn these probabilities into binary classifications, depending on whether or not they are >= 50%. Then, we will compare these estimates with the actual classes in the test data (i.e., test_dat$science_perf) in order to create a confusion matrix.


```{r}

dt_pred <- mutate(dt_pred,
  science_perf = as.factor(ifelse(High >= 0.5, "High", "Low")))
  
confusionMatrix(dt_pred$science_perf, test_dat$science_perf)

```

#### The output shows that the overall accuracy is around 92% , sensitivit is 94% , and specificity is 89%.

For only two variables, this is very good. However, sometimes we do not have predictors that are highly correlated with the outcome variables. In such cases, the model tuning might take much longer. Assume that we did NOT have reading and math in our data set. We still want to predict `science_perf` using the remaining variables.

```{r}

dt_fit3a <- rpart(formula = science_perf ~ WEALTH + HEDRES + ENVAWARE + ICTRES + EPIST + 
                   HOMEPOS +ESCS,
                 data = train_dat,
                 method = "class", 
                 control = rpart.control(minsplit = 20, 
                                         cp = 0.001, 
                                         xval = 0),
                parms = list(split = "gini"))

rpart.plot(dt_fit3a, extra = 8, box.palette = "RdBu", shadow.col = "gray")

```

####  Change cp to 0.005

```{r}
dt_fit3b <- rpart(formula = science_perf ~ WEALTH + HEDRES + ENVAWARE + ICTRES + EPIST + 
                   HOMEPOS + ESCS,
                 data = train_dat,
                 method = "class", 
                 control = rpart.control(minsplit = 20, 
                                         cp = 0.005, 
                                         xval = 0),
                parms = list(split = "gini"))

rpart.plot(dt_fit3b, extra = 8, box.palette = "RdBu", shadow.col = "gray")
```
#### Since we also care about the accuracy, sensitivity, and specificity of these models, we can turn this experiment into a small function

```{r}
decision_check <- function(cp) {
  require("rpart")
  require("dplyr")
  
  dt <- rpart(formula = science_perf ~ WEALTH + HEDRES + ENVAWARE + ICTRES + EPIST + 
                   HOMEPOS + ESCS,
              data = train_dat,
              method = "class", 
              control = rpart.control(minsplit = 20, 
                                           cp = cp, 
                                           xval = 0),
              parms = list(split = "gini"))
  
  dt_pred <- predict(dt, test_dat) %>%
    as.data.frame() %>%
    mutate(science_perf = as.factor(ifelse(High >= 0.5, "High", "Low"))) 
  
  cm <- confusionMatrix(dt_pred$science_perf, test_dat$science_perf)
  
  results <- data.frame(cp = cp, 
                        Accuracy = round(cm$overall[1], 3),
                        Sensitivity = round(cm$byClass[1], 3),
                        Specificity = round(cm$byClass[2], 3))
  
  return(results)
}

result <- NULL
for(i in seq(from=0.001, to=0.08, by = 0.005)) {
  result <- rbind(result, decision_check(cp = i))
}

result <- result[order(result$Accuracy, result$Sensitivity, result$Specificity),]
result
```

#### Visulize the results using ggplot2. First, we wil transform the result dataset into a long format and then use this new dataset (called result_long) in ggplot()

```{r}

result_long <- melt(as.data.table(result),
                    id.vars = c("cp"),
                    measure = c("Accuracy", "Sensitivity", "Specificity"),
                    variable.name = "Index",
                    value.name = "Value")

ggplot(data = result_long,
       mapping = aes(x = cp, y = Value)) +
  geom_point(aes(color = Index), size = 3) +
  labs(x = "Complexity Parameter", y = "Value") +
  theme_bw()
```
#### In the plot above, we see that there is a trade-off between sensitivity and specificity. 
Depending on the situation, we may prefer higher sensitivity (e.g., correctly identifying those who have “high” science scores) or higher specificity (e.g., correctly identifying those who have “low” science scores). For example, if we want to know who is performing poorly in science (so that we can design additional instructional materials), we may want the model to identify “low” performers more accurately.



## 7.2.1 Cross-validation

A typical way to use cross-validation in decision trees is to not specify a cp (i.e., complexity parameter) and perform cross validation. In the following example, we will assume that our dataset is not too big and thus we want to run 10 cross-validation samples (i.e., splits) as we build our decision tree model. Note that we use cp = 0 this time

```{r}

dt_fit4 <- rpart(formula = science_perf ~ WEALTH + HEDRES + ENVAWARE + ICTRES + 
                   EPIST + HOMEPOS + ESCS,
                 data = train_dat,
                 method = "class", 
                 control = rpart.control(minsplit = 20,
                                         cp = 0,
                                         xval = 10),
                parms = list(split = "gini"))
```


In the results, we can evaluate the cross-validated error (i.e., X-val Relative Error) and choose the complexity parameter that would give us an acceptable value. Then, we can use this cp value and prune the trees. 


```{r}
printcp(dt_fit4)
```

#### We use `plotcp() `function to visualize the cross-validation results.

Results indicate using cp = 0.0039. Researchers recommend that the ideal value of cp to select is at or below the median line. 


```{r}
plotcp(dt_fit4)
```


#### Next, we can modify our model as follows:

Include the selected value of `cp` from the cross validation results above. 
```{r}
dt_fit5 <- rpart(formula = science_perf ~ WEALTH + HEDRES + ENVAWARE + ICTRES + 
                   EPIST + HOMEPOS + ESCS,
                 data = train_dat,
                 method = "class", 
                 control = rpart.control(minsplit = 20, 
                                         cp = 0.0039,
                                         xval = 0),
                parms = list(split = "gini"))

printcp(dt_fit5)
```

#### Visualize the results 

```{r}
rpart.plot(dt_fit5, extra = 8, box.palette = "RdBu", shadow.col = "gray")
```

#### Lastly, we demonstrate a short regression tree example below where we predict math scores (a continuous variable) using the same set of variables. 

This time use `method = "anova"` in the `rpart()` function to estimate a regression tree.

Begin with cross-validation and check how R^2 changes depending on the number of splits.

```{r}
rt_fit1 <- rpart(formula = math ~ WEALTH + HEDRES + ENVAWARE + 
                  ICTRES + EPIST + HOMEPOS + ESCS,
                 data = train_dat,
                 method = "anova", 
                 control = rpart.control(minsplit = 20,
                                         cp = 0.001,
                                         xval = 10),
                parms = list(split = "gini"))

printcp(rt_fit1)

plotcp(rt_fit1) 
```

#### Then, adjust  model based on the suggestions from the previous plot. 

```{r}

rt_fit2 <- rpart(formula = math ~ WEALTH + HEDRES + ENVAWARE + 
                  ICTRES + EPIST + HOMEPOS + ESCS,
                 data = train_dat,
                 method = "anova", 
                 control = rpart.control(minsplit = 20, 
                                         cp = 0.007,
                                         xval = 0),
                parms = list(split = "gini"))

printcp(rt_fit2)

```

#### Visualize. Note that we use `extra = 100` in the `rpart.plot()` function to show percentages 
```{r}
rpart.plot(rt_fit2, extra = 100, box.palette = "RdBu", shadow.col = "gray")
```

#### To evaluate the model accuracy, we cannot use the classification-based indices anymore because we built a regression tree, not a classification tree.

Two useful measures that we can for evaluating regression trees are the mean absolute error (mae) and the root mean square error (rmse).The `modelr` package has several functions – such as `mae()` and `rmse()` – to evaluate regression-based models. 

Using the training and (more importantly) test data, we can evaluate the accuracy of the decision tree model that we estimated above.

```{r}
# Training data
mae(model = rt_fit2, data = train_dat)
```

```{r}
rmse(model = rt_fit2, data = train_dat)
```

```{r}
# Test data
mae(model = rt_fit2, data = test_dat)
```

```{r}
rmse(model = rt_fit2, data = test_dat)
```


#### We seem to have slightly less error with the training data than the test data. 
This finding is not surprising as the model will have a tendency to overfit on the training data (producing less of an error) than the test data that it is less familiar with. 


## 7.4 Random Forests

In the following example, we will focus on the same classification problem that we used before for decision trees.
We initially set `ntree = 1000` to get 1000 trees in total but we will evaluate whether we need all of these trees to have an accurate model.

```{r}
library("randomForest")
library("caret")

rf_fit1 <- randomForest(formula = science_perf ~ .,
                        data = train_dat,
                        importance = TRUE, ntree = 1000)

print(rf_fit1)

```

#### Visualize
```{r}
plot(rf_fit1)
```

#### The plot above shows that the error level does not go down any further after roughly 50 trees. 
So, we can run our model again by using `ntree = 50` this time.
```{r}
rf_fit2 <- randomForest(formula = science_perf ~ .,
                        data = train_dat,
                        importance = TRUE, ntree = 50)

print(rf_fit2)

```

#### We can see the overall accuracy of model (92.42% ) as follows:

```{r}
sum(diag(rf_fit2$confusion)) / nrow(train_dat)
```

#### As we did for the decision trees, we can check the importance of the predictors in the model, using `importance() `and `varImpPlot()`. 
With `importance()`, we will first import the importance measures, turn it into a data.frame, save the row names as predictor names, and finally sort the data by `MeanDecreaseGini`
```{r}
importance(rf_fit2) %>%
  as.data.frame() %>%
  mutate(Predictors = row.names(.)) %>%
  arrange(desc(MeanDecreaseGini))
```

### Visualize

```{r}

varImpPlot(rf_fit2, 
           main = "Importance of Variables for Science Performance")
```

#### The output above shows different importance measures for the predictors that we used in the model. 
MeanDecreaseAccuracy and MeanDecreaseGini represent the overall classification error rate (or, mean squared error for regression) and the total decrease in node impurities from splitting on the variable, averaged over all trees. In the output, math and reading are the two predictors that seem to influence the model performance substantially, whereas EPIST and HEDRES are the least important variables. `varImpPlot() `presents the same information visually.


#### Next, we check the confusion matrix to see the accuracy, sensitivity, and specificity of our model.

```{r}
rf_pred <- predict(rf_fit2, test_dat) %>%
  as.data.frame() %>%
  mutate(science_perf = as.factor(`.`))
  
confusionMatrix(rf_pred$science_perf, test_dat$science_perf)
```
#### The results show that the accuracy is quite high (92%)

Similarly, sensitivity (94%) and specificity (90%) are also very high. This is not necessarily surprising because we already knew that the math and reading scores are highly correlated with the science performance. Also, our decision tree model yielded very similar results.


#### Finally, let’s visualize the classification results using ggplot2. 
First,  create a new dataset called rf_class with the predicted and actual classifications (from the test data) based on the random forest model.Then,  visualize the correct and incorrect classifications using a bar chart and a point plot with jittering.
```{r}
rf_class <- data.frame(actual = test_dat$science_perf,
                      predicted = rf_pred$science_perf) %>%
  mutate(Status = ifelse(actual == predicted, TRUE, FALSE))

ggplot(data = rf_class, 
       mapping = aes(x = predicted, fill = Status)) +
  geom_bar(position = "dodge") +
  labs(x = "Predicted Science Performance",
       y = "Actual Science Performance") +
  theme_bw()
```

```{r}

ggplot(data = rf_class, 
       mapping = aes(x = predicted, y = actual, 
                     color = Status, shape = Status)) +
  geom_jitter(size = 2, alpha = 0.6) +
  labs(x = "Predicted Science Performance",
       y = "Actual Science Performance") +
  theme_bw()
```
#### Like decision trees, random forests can also be used for cross-validation, using the package rfUtilities that utilizes the objects returned from the `randomForest() `function. 

Below we show how cross-validation would work for random forests. Using the randomForest object that we estimated earlier (i.e.,, rf_fit2), we can run cross validations as follows:

```{r, message = FALSE}

library("rfUtilities")
rf_fit2_cv <- rf.crossValidation(
  x = rf_fit2, 
  xdata = train_dat,
  p=0.10, # Proportion of data to test (the rest is training)
  n=10,   # Number of cross validation samples
  ntree = 50)   


# Plot cross validation verses model producers accuracy
par(mfrow=c(1,2)) 
plot(rf_fit2_cv, type = "cv", main = "CV producers accuracy")
plot(rf_fit2_cv, type = "model", main = "Model producers accuracy")
par(mfrow=c(1,1)) 

# Plot cross validation verses model oob
par(mfrow=c(1,2)) 
plot(rf_fit2_cv, type = "cv", stat = "oob", main = "CV oob error")
plot(rf_fit2_cv, type = "model", stat = "oob", main = "Model oob error")    
par(mfrow=c(1,1)) 
```







# Part III 

#### Provide a 2-3 page single spaced brief research proposal in which you argue for and justify the use of ROC AUC accuracy analysis and/or regression trees applied to a research topic that you are interested in. Please address the following questions in this order as you apply your new knowledge of these techniques (feel free to write this part in MS Word or similar and copy/paste into the markdown)

*a. What is the purpose of your study? *
	
	The purpose of this study is to apply CART analysis to predict which employees are likely to voluntarily leave their job. By using machine learning (ML), this study will train an ML model on prior employee turnover records to produce turnover likelihood scores for current employees. In addition, through the use of ROC AUC analysis and regression tree models (CART), this study will identify the strongest predictors of voluntary turnover. 

*b. Is there any research literature and theory that supports this argument? How so? *

   Past literature suggests that numerous variables are related to turnover decisions. Due to the complexity behind turnover decisions, it is beyond the scope of this research proposal to discuss them all. That being said, variables that have been shown to predict turnover include job satisfaction, burnout, perception of leadership, and disengagement at work. 

   To explore key predictors of voluntary turnover, this study hopes to analyze either the IBM HR Analytics Employee Attrition & Performance data set or the Work, Family and Health Network data set. The IBM data set contains attrition rate, as well as demographic and work environments variables, including age, business travel demands, gender, job satisfaction, environment satisfaction, education field, job role, income, overtime, percentage salary hike, tenure, training time, years in current role, relationship status, work life balance, distance from home, years since last promotion, and more. While the Work, Family and Health data set includes variables such as intention to quit, control over work schedule, burnout, job strain, low value work, work-family conflict, job satisfaction, and more. 

*c. Why is ROC AUC accuracy and/or regression trees (or both) a means to address this purpose? *
	
   CART analysis allows us to use the available data to identify the relationship between employees and work characteristics and turnover outcomes. By applying a CART analysis, the decision tree can then be applied to current employees to identify employees who are at risk of quitting. This analysis can help inform organizations which current employees might benefit from additional intervention or support to prevent attrition. Furthermore, the use of CART models will allow us to visualize variable importance in the prediction of turnover. Doing so will allow us to identify the most important predictors of turnover. In addition, CART models will allow us to identify the proportion of participants in our data set that are predicted to have specific levels of a given variable based on the variable cut point. For example, we anticipate that employee engagement will be a key predictor of turnover. CART will partition the employee engagement variable using gini index calculations to determine which level of engagement meaningfully separates those that turnover and those that do not turnover. For those that do turnover, we will want to know how many participants from our overall sample are in the sub-sets corresponding to specific levels of engagement. Doing so will contribute to our overall understanding of which variables predict turnover the best. 


*d. What would be the research question(s)? (To what extent…) *

This study hopes to identify the most important predictors of voluntary employee turnover and to identify current employees that are most at risk of turning over. 

*e. What type of data set would you need? Is there a data set you know of that would work? *
	
   We will need a data set with numerical and/or categorical variables that also contains historical turnover records for employees. As mentioned above, we will work with either the IBM HR Analytics Employee Attrition & Performance data set or the Work, Family and Health data set, both of which satisfy these requirements. The advantages of the IBM data set are 1) unlike the Work, Family and Health data set, it includes demographic variables, which had been found to relate to voluntary decisions 2) it contains turnover outcome, whereas the Work, Family and Health data set only measures intention to quit. On the other hand, the Work, Family and Health data set contains various psychological and work environment variables. Most importantly, the Work, Family and Health data set is real, whereas the IBM data set is fictional. 

*f. What types of data would you be looking for? *

We can use continuous and categorical variables as predictors. Turnover will be a binary variable. 

*g. Provide the generalized equation for the relevant underlying mathematical functions and a brief narrative in which you specify the type of calculation following the examples from the readings. *
	
In CART, the data are split into sub-nodes in each decision node based on the best predictor and its threshold value. CART algorithm splits subsequent sub-nodes with the help of the Gini or entropy indices.

According to Bulut and Desjardins (2021):

“When building a classification tree, either the Gini index or the entropy is typically used to evaluate the quality of a particular split, as they are more sensitive to the changes in the splits than the classification error rate. Typically, the Gini index is better for minimizing mis-classification, while the Entropy is better for exploratory analysis.

Gini Index is calculated as follows: 

```{r, echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics("gini_index.png")
```
where KK represents the number of classes. This is essentially a measure of total variance across the KK classes. A small Gini index indicates that a node contains predominantly observations from a single class (Bulut & Desjardins, 2021). 

Entropy can be calculated as follows: 

```{r pressure, echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics("entropy_equation.png")
```




*h. What do you think you would find? *

   We believe we will find which employees are at risk of voluntarily leaving their job and the pattern of variables associated with this risk. The specific variables will depend on the data set we pick. 
 
*i. Why would this be important? What would be the implications for this research domain? *
	
   Employee turnover is a workplace outcome that has significantly interested researchers and practitioners for decades due to the high cost and negative effects associated with turnover. For example, recruiting for vacant positions is time intensive and on-boarding new hires is costly. By applying CART analysis to predict which employees are at risk of quitting, we intent to contribute to this rich and important area of research in which organizations can benefit.



