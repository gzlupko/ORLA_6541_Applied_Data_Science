---
title: "Exercise 4"
author: "Nilima Ajaikumar, Hagay Jalon & Gian Zlupko"
date: "2022-11-14"
output: html_document
---

```{r message = FALSE, warning = FALSE}
library(tidyverse) 
library(xlsx) 
library(pROC) 
library(OptimalCutpoints) # for dealing with cutoff point
library(naniar) # replaces missing data to NA
```


# Part I: NECS Data 

For the first part of our Exercise 4, we are recreating the figures and tables published in Bowers & Zhou (2019). Our code and results for this section are organized as follows. First, we load and clean the NCES data. Then, we provide the code, documentation, and interpretation for Tables 1-3 and Figures 1-3 from Bowers & Zhou (2019) in the order in which they appear in the article. 

#### Import Data 

The following steps were implemented to access data from NCES' online repository. 


```{r}
setwd("/Users/gianzlupko/Desktop/ORLA 6541 Data Science/ORLA_6541_Applied_Data_Science/NCES_Data") 

# Load R Data File
load("els_full_data.rdata")

# simplify name of data frame 
els_full_data <- els_02_12_byf3pststu_v1_0


# variables that we would like to keep 

keepvars <- c(
   "STU_ID",
   "F2EVERDO", # Dropout
   "BYP52E", # Absent*
   "BYS24F", # Suspension*
   "BYP51", # Misbehavior*
   "BYTXMSTD", # Math t score (2002) - lowercase in the data
   "BYS42", # Extracurricular activities (2002) hours/wk
   "BYTXRSTD", # Reading t score (2002)* - lowercase in the data
   "F2PS0601", # College enrollment
   "F1RGPP2", # GPA
   "F1S27", # Extracurricular activities (2004) hours/wk
   "BYS33A", # AP
   "F3TZSTEM1CRED", # Postsecondary STEM degree
   "F3TZSTEM1TOT", # Number of STEM Courses
   "F3TZSTEM2GPA", # STEM course GPA
   "F3STEMOCCCUR" # Hard STEM career
)

# select subset of variables that were used in Bowers & Zhou (2019)
# also apply renaming to make the data readable 
els_clean <- els_full_data %>% 
  select(STU_ID, F2EVERDO, BYP52E, BYS24F, BYP51, bytxmstd, BYS42, bytxrstd,
         F2PS0601, F1RGPP2, F1S27, BYS33A, F3TZSTEM1CRED, F3TZSTEM1TOT,
         F3TZSTEM2GPA, F3STEMOCCCUR) %>%
  rename(student_ID = STU_ID, drop_out = F2EVERDO, absent = BYP52E, suspension = BYS24F, 
         misbehavior = BYP51, math_tscore = bytxmstd, extra_curr_2002 = BYS42, 
         reading_tscore = bytxrstd, college_enroll = F2PS0601, GPA = F1RGPP2, extra_curr_2004 = F1S27, AP = BYS33A, post_sec_stem = F3TZSTEM1CRED, no_stem_courses = F3TZSTEM1TOT, stem_GPA = F3TZSTEM2GPA, hard_stem_career = F3STEMOCCCUR)   

# inspect cleaning 
els_clean


# finally, create the feature engineered flags that Bowers & Zhou also tested to look at 
# the impact of the potential implication of one or more 'flags' or common indicators of 
# student drop out 
# the flags include: absent, suspension math_tscore in the 1st percentile, and misbehavior 

els_vars <- els_clean %>%
  mutate(flag_00 = ifelse(rowSums(select(., c(3:6)), na.rm = T) >= 1, 1, 0), 
         flag_01 = ifelse(absent == 1 | suspension == 1 | math_tscore == 1 | misbehavior == 1, 1,0), flag_04 = ifelse(absent == 1 & suspension == 1 & math_tscore == 1 & misbehavior == 1, 1, 0))

```

Remove missing data in the data set that are stored as -9, -8, -4, -1

```{r}
library(naniar)
els_complete <- replace_with_na_if(els_vars,
                                   .predicate = is.numeric,
                                   condition = ~.x < 0)

# Create the math t-score variable following Bowers' CART markdown 

els_complete <- els_complete %>% 
  mutate(math_t_score = ifelse(math_tscore < 44.065, 1, 0), 
         reading_t_score = ifelse(reading_tscore < 43.64, 1, 0))


# finally, view summary of clean variables 
summary(els_complete) 
```


Following Bowers & Zhou (2019), the first table that we create below is Figure 1, which presents a confusion table that specifies common metrics used in machine learning to evaluate prediction performance. 



### Table 1: Confusion Table for Calculating Contingency Proportions 


```{r}

fig_1_stats <- c("Accuracy", "Precision", "Sensitivity (Recall / True-positive Proportion",
                  "Specificity (True-negative Proportion", "1-Specificity (False-positive Proportion", "Kapp")

fig_1_equations <- c("(a + d) / N", 
                     "a / (a + b)", 
                     "a / (a + c)", 
                     "d / (b + d)", 
                     "b / (b + d)", 
                     "(Accuracy - R) / (1 - R)")

# bind columns 
fig_1 <- data.frame(cbind(fig_1_stats, fig_1_equations)) 
colnames(fig_1) <- c("Metric", "Equation")
fig_1

```




### Figures 2A and 2B 

Figure A depicts a ROC plot that utilizes predictors used by Balfanz et al.  (2007) and Figure B uses predictors selected by Bowers & Zhou (2019). 


Figure 2A
```{r, message = FALSE}

# ROC for math t-score
roc1 = roc(els_complete$drop_out, els_complete$math_t_score, legacy.axes=TRUE, asp=FALSE, plot=TRUE, grid=FALSE, lty=1, xaxs="i", yaxs="i", main = "ROC curves for predicting dropout")

# ROC for one flag 
roc2 = roc(els_complete$drop_out, els_complete$flag_01, plot=TRUE, add=TRUE, percent=roc1$percent, lty=2)

# ROC for any number of flags 
roc3 = roc(els_complete$drop_out, els_complete$flag_00, plot=TRUE, add=TRUE, percent=roc1$percent, lty=3)

# ROC for all four
roc4 = roc(els_complete$drop_out, els_complete$flag_04, plot=TRUE, add=TRUE, percent=roc1$percent, lty=4)

# ROC for suspension 
roc5 = roc(els_complete$drop_out, els_complete$suspension, plot=TRUE, add=TRUE, percent=roc1$percent, lty=5)

# ROC for misbehavior
roc6 = roc(els_complete$drop_out, els_complete$misbehavior, plot=TRUE, add=TRUE, percent=roc1$percent, lty=6)

# ROC for absent
roc7 = roc(els_complete$drop_out, els_complete$absent, plot=TRUE, add=TRUE, percent=roc1$percent, lty=7)

# legend for the plot 
legend("bottomright", legend=c("Math t score (2002)","Any No. Flags","All Four Flags","Suspension", "Misbehavior", "Absent"), lty=c(1,2,3,4,5,6,7), lwd=2)

```


Figure 2B: 
```{r, message = FALSE}

# ROC for math t-score
roc8 = roc(els_complete$drop_out, els_complete$math_t_score, legacy.axes=TRUE, asp=FALSE, plot=TRUE, grid=FALSE, lty=7, xaxs="i", yaxs="i", main = "ROC curves for predicting dropout")

# ROC for reading t score   
roc9 = roc(els_complete$drop_out, els_complete$reading_t_score, plot=TRUE, add=TRUE, percent=roc1$percent, lty=8)

# ROC for absent 
roc10 = roc(els_complete$drop_out, els_complete$absent, plot=TRUE, add=TRUE, percent=roc1$percent, lty=9)

# ROC for suspension 
roc11 = roc(els_complete$drop_out, els_complete$absent, plot=TRUE, add=TRUE, percent=roc1$percent, lty=10)

# ROC for suspension 
roc12 = roc(els_complete$drop_out, els_complete$extra_curr_2002, plot=TRUE, add=TRUE, percent=roc1$percent, lty=11)


# legend for the plot 
legend("bottomright", legend=c("Math t score (2002)", "Reading t score (2002)", "Extrac. activities (2002)", "Absence", "Suspension"), lty=c(7,8,9,10,11), lwd=2)
  
  
```

Figure 2A and 2B depict ROC curves for key variables used to predict high school drop out. All of the predictors in Figure 2B predicted drop out better than chance. This result can be seen by visually inspecting each ROC curve and their distance from the diagonal line in the center of the plot, which indicates a random guess of 0.5. In contrast to Figure 2B, some of the predictors visualized in Figure 2A did not perform better than chance at predicting drop out. In particular, results in Figure 2A demonstrate that any number of flags performed worse than a random guess and all four flags performed about equivalent to a random guess in predictive capacity. 



##### AUC Values Corresponding with Figure 2A and 2B 

Below, we create the data table that is pictured below Figures 2A and 2B in Bowers & Zhou (2019) to show the AUC values that correspond with their respective ROC curves. 

Table under 2A: 
```{r}
# print AUC values and store to a vector
auc_fig2a <- c(roc1$auc, roc2$auc, roc3$auc, roc4$auc, roc5$auc, roc6$auc, roc7$auc) 

# store predictor names in a vector
predictor_fig2a <- c("Math t score (2002)", "Any one flag", "Any number of flags", "All four flags", "Suspension", 
                     "Misbehavior", "Absent")  

# combine vector into a data frame, rename columns, adjust decimal points, and sort desceding by top AUC

fig2a_table <- data.frame(cbind(predictor_fig2a, auc_fig2a)) 
colnames(fig2a_table) <- c("Predictor", "AUC") 

# roundabout convert from factor, have to go through character format first
fig2a_table$AUC <- as.numeric(as.character(fig2a_table$AUC)) 

fig2a_table %>% 
   dplyr::mutate_if(is.numeric, round, 2) 

```

Table under 2B: 
```{r}

# print AUC values and store to a vector
auc_fig2b <- c(roc8$auc, roc9$auc, roc10$auc, roc11$auc, roc12$auc) 

# store predictor names in a vector
predictor_fig2b <- c("Math t score (2002)", "Reading t score (2002)", "Exac. activities (2002)", 
                     "Absence", "Suspension") 

# combine vector into a data frame, rename columns, adjust decimal points, and sort desceding by top AUC

fig2b_table <- data.frame(cbind(predictor_fig2b, auc_fig2b)) 
colnames(fig2b_table) <- c("Predictor", "AUC") 

# roundabout convert from factor, have to go through character format first
fig2b_table$AUC <- as.numeric(as.character(fig2b_table$AUC)) 

fig2b_table %>% 
   dplyr::mutate_if(is.numeric, round, 2) 
```


Results from tables corresponding with Figures 2A and 2B indicated that math t score (2002) was the best predictor of high school drop out. Other predictors that performed well in predicting drop out were absences and involvement in extracurricular activities. 


#### Table 1: Significance of AUC Difference for Predictors of Continuous Dropout 

In our recreation of Table 1 below, we present results from a Delong test of the difference of two correlated ROC curves. Large Z scores and small p-values (below the conventional p < .05) indicate that the area under the respective curves is significantly different. For our table, we compared each predictor to math t score (2002) which was the predictor with the best AUC value observed on the current data. 

```{r}

# custom function to extra Z scores and p-values 
roc_sig_diff <- function(x, y) { 
   
test1 <- roc.test(x,y) 
print(test1$p.value) 
print(test1$statistic) 
   
   }

# cycle through all comparisons needed to build the table: 
roc_sig_diff(roc1, roc2)  
roc_sig_diff(roc1, roc3)  
roc_sig_diff(roc1, roc4)  
roc_sig_diff(roc1, roc5)  
roc_sig_diff(roc1, roc6)  
roc_sig_diff(roc1, roc7)  

# create columns for table 

pred_list_1 <- c("Math t score (2002)", "Math t score (2002)", "Math t score (2002)",
                 "Math t score (2002)", "Math t score (2002)", "Math t score (2002)")  

pred_list_2 <- c("One Flag", "Any number of flags", "All four flags", "Suspension", "Misbehavior", 
                 "Absence")

# create z score column 

z_scores <- c(26.78, 23.85, 23.85, 8.55, 6.45, 0.62)

# create p-value column 
p_values <- c("<.001","<.001", "<.001", "<.001", "0.53", "0.61")


# combine all columns and rename columns 

table_1 <- data.frame(cbind(pred_list_1, pred_list_2, z_scores, p_values)) 
table_1 %>% 
   rename("Predictor 1 " = pred_list_1, "Predictor 2" = pred_list_2, "Z" = z_scores, "p_value" = p_values)
```



Results in Table 1 show that math t score (2002) performed significantly better at predicting dropout than one flag, any number of flags, all four flags, and suspension. In contrast, the results also demonstrated that math t score did not have a significantly different AUC than misbehavior and absence.







