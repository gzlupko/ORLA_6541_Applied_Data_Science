---
title: "ORLA 6541 - Exercise 1"
author: "Nilima Ajaikumar, Hagay Jalon, and Gian Zlupko"
date: '2022-10-03'
output: html_document
---

ORLA 6541 - Exercise 1 

The following code and write-up contains answers to in-text exercises in
Wickham & Grolemund (2017): Chapters 3-6 and Bulut & Dejardins (2019) Chapter 4. 

# Wickham & Grolemund (2017):
### Chapter 3: Data Visualization 


## 3.2.4 Exercises

#### 1.	Run ggplot(data = mpg). What do you see?
A blank grey square

```{r, message = FALSE}
library(tidyverse)

ggplot(data=mpg)
```

#### 2.	How many rows are in mpg? How many columns?
234 rows and 11 columns

```{r}
mpg
```
#### 3. What does the drv variable describe? Read the help for ?mpg to find out.
The drv variable describes the type of drive train for each car in the data set, where f = front-wheel drive, r = rear wheel drive, 4 = 4wd. 

```{r}
?mpg
```

#### 4.	Make a scatterplot of hwy vs cyl.
```{r}
ggplot(data = mpg)+geom_point(mapping=aes(x=cyl, y=hwy))
```

#### 5.	What happens if you make a scatterplot of class vs drv? Why is the plot not useful?

Since both variables are categorical, the points align based on the intersection of both categories, which does not provide any useful information. Scatterplots are more useful for mapping the relationship between two continuous variables. 

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = class, y = drv))
```
## 3.3.1 Exercises
#### 1.	What’s gone wrong with this code? Why are the points not blue? 
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = "blue"))
  
  aes() is used to map an aesthetic to a variable.  In this case, we wanted to associate the aesthetic color and with the actual color “blue”. Since “blue” is not a variable, we cannot map it in aesthetic, and had to set the aesthetic properties of our geom manually (outside of aes() . As such: 


```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), color = "blue")
```


#### 2.	Which variables in mpg are categorical? Which variables are continuous? (Hint: type ?mpg to read the documentation for the dataset). How can you see this information when you run mpg?

```{r}
?mpg
```

Categorical: manufacturer, model, trans, drv, fl, class
Continuous: displ, year, cyl, cty, hwy

You can see this information when running ?mpg under each variable's name

#### 3.	Map a continuous variable to color, size, and shape. How do these aesthetics behave differently for categorical vs. continuous variables?

Assigning color to a continuous variable provides a numerical scale of colors while assigning color to a categorical variable categorizes all levels of the variable by different colors.

Similarly, assigning size to a continuous variable provides a numerical scale of sizes, while assigning size to a categorical variable (although not advised), categorizes all levels of the variable by different sizes.

Continuous variables can not be mapped to shape. While assigning shape to a categorical variable, all levels of the variable by different shapes (with a maximum of 6 discrete values because more than 6 becomes difficult to discriminate)


```{r}
ggplot(data = mpg) + 
    geom_point(mapping = aes(x = displ, y = hwy, color = cty))
```

```{r}
ggplot(data = mpg) + 
    geom_point(mapping = aes(x = displ, y = hwy, size = cty))
```

#### 4.	What happens if you map the same variable to multiple aesthetics? 

You get a scatterpllot with an overlap of two aesthetics, both convey the same information. 

```{r}
ggplot(data = mpg) + 
    geom_point(mapping = aes(x = displ, y = hwy, color = cty, alpha=cty))
```

#### 5.	What does the stroke aesthetic do? What shapes does it work with? (Hint: use ?geom_point)

It is used to modify the width of the border of shapes with borders. 

```{r}
?geom_point
```


#### 6.	What happens if you map an aesthetic to something other than a variable name, like aes(colour = displ < 5)? Note, you’ll also need to specify x and y.

It categorizes the variable assigned to the x-axis (in this case, displ) to < or > 5 by two different colors. 

```{r}
ggplot(data = mpg) + 
    geom_point(mapping = aes(x = displ, y = hwy, color = displ<5))
```

## 3.5.1 Exercises

#### 1.	What happens if you facet on a continuous variable?

We get a new column for each value of the variable.

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ cty, nrow = 2)
```


#### 2.	What do the empty cells in plot with facet_grid(drv ~ cyl) mean? How do they relate to this plot?


```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ cyl)
```
The empty cells in `facet_grid` suggest that the variables did not intersect within these values. This relates to the first plot because we can notice that in the first plot, there is much empty space and if we were to facet it, we are likely to have empty cells as well.


#### 3.	What plots does the following code make? What does . do?

`.` allows us to facet the plot by all levels of the variable provided without needing to specify the levels. 

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) +
  facet_grid(drv ~ .)

```


```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) +
  facet_grid(. ~ cyl)
```

#### 4. Take the first faceted plot in this section: What are the advantages to using faceting instead of the colour aesthetic? What are the disadvantages? How might the balance change if you had a larger dataset?


```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 2)
```
Advantages: it is easier to identify patterns within each value of a variable; we can have a larger number of facets while still being able to identify which facet represents which value, while with a large number of colors, it may be  difficult to discriminate between values. 
Disadvantages: It may be more challenging to compare patterns between values of the variable and to identify overall patterns.

#### 5. Read ?facet_wrap. What does nrow do? What does ncol do? What other options control the layout of the individual panels? Why doesn’t facet_grid() have nrow and ncol arguments?

```{r}
?facet_wrap
```
`nrow` determines the number of rows in the facet. Other options that control the layout of the individual panels are: `ncol`, scale, and dir. `ncol` determines the number of columns in the facet. `facet_grid`() does not have number of `nrow` and `ncol` arguments because they are determined by the number of unique levels the variables have.

#### 6. When using facet_grid() you should usually put the variable with more unique levels in the columns. Why?

Because if you put the variable with the more unique levels in the rows, it would create a taller then wider plot, which may be hard to follow and interpret on most computers.

## 3.6.1 Exercises

#### 1. What geom would you use to draw a line chart? A boxplot? A histogram? An area chart?

line chart - `geom_line()`
boxplot - `geom_boxplot()`
histogram - `geom_histogram()`


#### 2. Run this code in your head and predict what the output will look like. Then, run the code in R and check your predictions.

This will create a scatter plot with displ as the x variable and hwy as the y variable. drv will be represented by both lines and errors colored by the levels of the variable, and their standard error would not appear.

```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + 
  geom_point() + 
  geom_smooth(se = FALSE)
```


#### 3. What does show.legend = FALSE do? What happens if you remove it? Why do you think I used it earlier in the chapter?

show.legend = FALSE hides the legend box in the of the plot. It plausible that it was removed to ensure the plot's size is consistent with previous plots.


#### 4. What does the se argument to geom_smooth() do?

It visulizes the confidence intervals of the line as shaded areas; TRUE to show, FALSE to hide.

#### 5. Will these two graphs look different? Why/why not?


```{r, message = FALSE}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point() + 
  geom_smooth()
```


```{r, message = FALSE}
ggplot() + 
  geom_point(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_smooth(data = mpg, mapping = aes(x = displ, y = hwy))
```

No, they will be identical since Because both geom_point() and geom_smooth() will uses the same data and mapping noted in the ggplot(), which is the same for both plots. 

#### 6. Recreate the R code necessary to generate the following graphs.


```{r, message = FALSE}
ggplot(data=mpg, mapping=aes(x = displ, y = hwy,)) +
  geom_point(size=5) +
  geom_smooth(se = FALSE, size=2)
```


```{r, message = FALSE}
 ggplot(data = mpg, mapping = aes(x = displ, y = hwy))+
     geom_smooth(mapping = aes(group = drv), se = FALSE, size=2) +
  geom_point(size=5)
```


```{r, message = FALSE}
 ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color=drv))+
     geom_smooth(se = FALSE, size=2) +
  geom_point(size=5)
```


```{r, message = FALSE}
ggplot(data = mpg, mapping = aes (x = displ, y = hwy))+
     geom_smooth(se = FALSE, size=2) +
  geom_point(mapping=aes(color=drv), size=5)
```


```{r, message = FALSE}
 ggplot(data = mpg, mapping = aes(x = displ, y = hwy))+
     geom_smooth(mapping=aes(linetype=drv), se = FALSE, size=2) +
  geom_point(mapping = aes(color=drv), size=5)
```

```{r, message = FALSE}
 ggplot(data = mpg, mapping = aes(x = displ, y = hwy))+
geom_point(size = 5, color = "white") +
  geom_point(aes(color = drv))
```

## 3.7.1 Exercises

#### 1. What is the default geom associated with stat_summary()? How could you rewrite the previous plot to use that geom function instead of the stat function?

```{r}
?stat_summary
```


the default geom associated with stat_summary is geom_pointrange().  
```{r}
ggplot(data = diamonds) + 
  stat_summary(
    mapping = aes(x = cut, y = depth),
    fun.min = min,
    fun.max = max,
    fun = median
  )
```


#### 2. What does geom_col() do? How is it different to geom_bar()?

```{r}
?geom_col
```

```{r}
?geom_bar
```

There are two types of bar charts: geom_bar() and geom_col(). geom_bar() makes the height of the bar proportional to the number of cases in each group (or if the weight aesthetic is supplied, the sum of the weights). If you want the heights of the bars to represent values in the data, use geom_col() instead. geom_bar() uses stat_count() by default: it counts the number of cases at each x position. geom_col() uses stat_identity(): it leaves the data as is.

#### 3. Most geoms and stats come in pairs that are almost always used in concert. Read through the documentation and make a list of all the pairs. What do they have in common?

Pairs of geoms and stats share names in common and typically have the same default stat.

#### 4. What variables does stat_smooth() compute? What parameters control its behaviour?

```{r}
?stat_smooth
```
Two parameters that control `stat_smooth()'`s behavior are `method` and `formula.` Method specifies the kind of smoothing function to apply, like a linear model (lm) or a generalized linear model (glm), and formula allows the user to provide the variables of interest for the given modeling strategy. In addition, users can also adjust the `fullrange` parameter, which tells R to either fit the smoothing for just the data or the full plot. 

stat_smooth() generates the following variables: 

y or x
predicted value of the given variable(s) 

ymin or xmin
lower pointwise confidence interval around the mean

ymax or xmax
upper pointwise confidence interval around the mean

se
standard error



#### 5.In our proportion bar chart, we need to set group = 1. Why? In other words what is the problem with these two graphs?

```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, y = after_stat(prop)))
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = color, y = after_stat(prop)))
```


The problems is that all of the bars are the same height, which is inaccurate. Setting group=1 ensures that the proportions for each stack are calculated using each subset. 
  
  
## 3.8.1 Exercises

#### 1. What is the problem with this plot? How could you improve it?


```{r}
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + 
  geom_point()
```
It is possible that many points in the plot overlap and, therefore, hide each other. We can improve the plot by using the geom_jitter() function to add a small amount of random noise to each point, which will reveal any overlapping points. As such:

  
```{r}
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + 
  geom_jitter()
```


#### 2. What parameters to geom_jitter() control the amount of jittering?

Width, controlling the horizontal random noise, and height, controlling the vertical random noise.
```{r}
?geom_jitter
```


#### 3. Compare and contrast geom_jitter() with geom_count().

```{r}
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + 
  geom_count()
```
While `geom_jitter` adds random noise to each point to reduce over plotting of points, `geom_count` changes the size of each point per the number of observation for its specific value. Although geom_count can help to reduce over plotting; however, it may create over plotting by itself due to the size of the points.


#### 4. What’s the default position adjustment for geom_boxplot()? Create a visualisation of the mpg dataset that demonstrates it.


```{r}
?geom_boxplot
```


The default position adjustment for geom_boxplot() is dodge2.

```{r}
ggplot(data = mpg, aes(x = drv, y = hwy)) +
  geom_boxplot()
```


## 3.9.1 Exercises

#### 1. Turn a stacked bar chart into a pie chart using coord_polar().

```{r}
ggplot(mpg, aes(x = class, fill = drv)) + geom_bar()
```


```{r}
ggplot(mpg, aes(x = class, fill = drv)) + geom_bar() + coord_polar()
```

#### 2. What does labs() do? Read the documentation.

```{r}
?labs()
```

`labs()` allows users to set plot labels. Within the `labs()` function, users can provide details for host of plot labels, like title, subtitle, caption, and more. 

#### 3. What’s the difference between coord_quickmap() and coord_map()?


```{r}
?coord_quickmap
```

`coord_map()` from ggplot2 allows users to map their data onto a 2D projection of the earth. Due to the curvature of the earth, the documentation notes that map projections do not preserve straigth lines. As such, `coord_quickmap()` can be used alternatively, which performs calculations to provide an approximation of the requested area of the globe but with straight lines. 


#### 4. What does the plot below tell you about the relationship between city and highway mpg? Why is coord_fixed() important? What does geom_abline() do?

The plot below suggests a strong positive relationship between city miles per gallon to highway miles per gallon; as one rises, so does that other. 

coord_fixed() is important because it ensures a fixed ratio between the physical representation of data points on the X and Y axes. This makes it easier to identify patterns in the plot.

geom_abline() adds a reference line to a plot, either horizontal, vertical, or diagonal, which is useful for annotating plots. In this case, it added a horizontal line to the plot, which made it easier to identify the relationship between city and highway mpg.


```{r}
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +
  geom_point() + 
  geom_abline() +
  coord_fixed()
```


```{r}
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +
  geom_point() + 
  geom_abline()
```


```{r}
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +
  geom_point() + 
  coord_fixed()
```


# Wickham & Grolemund Chapter 4 
### Workflow: basics


## 4.4 Exercises

#### 1. Why does this code not work?

my_variable <- 10
my_varıable
#> Error in eval(expr, envir, enclos): object 'my_varıable' not found

This code does not work because the "I" in the second row of the code is capital, which does not match the "i" above. 

#### 2. Tweak each of the following R commands so that they run correctly:

`library(tidyverse)`

`ggplot(dota = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))`

`fliter(mpg, cyl = 8)
filter(diamond, carat > 3)
`

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))
```

```{r}
filter(mpg, cyl == 8)
filter(diamonds, carat > 3)
```

#### 3. Press Alt + Shift + K. What happens? How can you get to the same place using the menus?

A menu with all shortcuts appear. I can access this menu through Tools, keyboard shortcuts helps.


```{r libaries, message = FALSE}
library(tidyverse) 
```





# Wickham & Grolemund Ch. 6 
### Workflow: scipts 

## 6.3 Exercises
#### 1. Go to the RStudio Tips twitter account, https://twitter.com/rstudiotips and find one tip that looks interesting. Practice using it!

Tip from: 
Dr. Rebecca Hirst @HirstR omg did you know you can access #R #cheatsheets straight from Rstudio?!

I’m heading back under my rock now 🙈 #coding #rstats

To follow Dr. Hirst's tip, in RStudio, go to: File > Help  > Cheat Sheets

#### 2. What other common mistakes will RStudio diagnostics report? Read https://support.rstudio.com/hc/en-us/articles/205753617-Code-Diagnostics to find out.
Warn if variable used has no definition in scope: Warn if a symbol is used with no definition in the current, or parent, scope. Warn if variable is defined but not used: helps to identify is a variable is created but never used. Provide R style diagnostics (e.g. whitespace): checks to see if your code conforms to Hadley Wickham’s style guide, and reports style warnings when encountered.


# Wickham & Grolemund Chapter 5 


# Load relevant packages

```{r package-setup, message = FALSE, warning = FALSE}

library(nycflights13)
library(tidyverse)

```

## 5.2.4


#### 1.1 Find all flights that: 
    - Had an arrival delay of two or more hours
    - Flew to Houston (IAH or HOU)
    - Were operated by United, American, or Delta
    - Departed in summer (July, August, and September)
    - Arrived more than two hours late, but didn’t leave late
    - Were delayed by at least an hour, but made up over 30 minutes in flight
    - Departed between midnight and 6am (inclusive)

- There were 10,200 flights that had an arrival delay of two or more hours

```{r, message = FALSE, warning = FALSE}

filter(flights, arr_delay >= 120)

```

#### 1.2. 

- 9,313 flights flew to Houston (IAH or HOU)

```{r, message = FALSE, warning = FALSE}

filter(flights, dest == "IAH" | dest == "HOU")

```

#### 1.3. 

- 139,504 flights were operated by United (UA), American (AA), or Delta (DL)

```{r, message = FALSE, warning = FALSE}

filter(flights, carrier %in% c("UA", "AA", "DL"))

```

#### 1.4. 

- 86,326 flights  departed in summer (July, August, and September)

```{r, message = FALSE, warning = FALSE}

filter(flights, month %in% c(7, 8, 9))

```

#### 1.5. 

- 29 flights arrived more than two hours late, but didn’t leave late

```{r, message = FALSE, warning = FALSE}

filter(flights, arr_delay > 120, dep_delay <= 0)

```

#### 1.6. 

- 206 flights were delayed by at least an hour, but made up over 30 minutes in flight

```{r, message = FALSE, warning = FALSE}

filter(flights, dep_delay >= 60 , arr_delay < 30)

```
#### 1.7. 

- 9373 flights departed between midnight and 6am (inclusive)

```{r, message = FALSE, warning = FALSE}

summary(flights$dep_time)

# max departure time is 2400, this represents midnight

filter(flights, dep_time <= 600 | dep_time == 2400)


```



#### 2. 

- The between(x, left, right) function allows you to filter values where x is greater than or equal to left and less than or equal to right. 

- The code for exercise 1.4 can be simplified using the between() function as follows:

```{r, message = FALSE, warning = FALSE}

# Simplifying code using between() function

filter(flights, between(month, 7, 9))

```


#### 3. 

- 8,255 flights have a missing departure time (dep_time). 
- For these flights the following details are also missing, departure delay (dep_delay), scheduled arrival time (arr_time), arrival delay (arr_delay)and amount of time spent in the air (air_time). 
- These are likely flights that never took off.

```{r, message = FALSE, warning = FALSE}

filter(flights, is.na(dep_time))

```

#### 4. 

 - NA^1 is not missing since mathematically any variable to the power of 0 is 1
 
 - NA | TRUE is true, since NA represents an unknown value and TRUE appears after the boolean operator "or", therefore it returns TRUE 
 
 -  NA & FALSE is false since NA represents an unknown value and FALSE appears after the boolean operator "and", therefore it returns FALSE

```{r, message = FALSE, warning = FALSE}

NA^0
NA | TRUE
NA & FALSE
NA *0

```

## 5.3.1

#### 1. 

- Using arrange() to sort all missing values to the start (within the dep_delay column)

```{r, message = FALSE, warning = FALSE}

arrange(flights, desc(is.na(dep_delay)), dep_delay)


```


#### 2. 

- Sorting flights to find the most delayed flights. 
- The most delayed flight was HA 51 traveling from JFK to HNL on Jan 09, 2013

```{r, message = FALSE, warning = FALSE}

# arrange dep_delay column in descending order

arrange(flights, desc(dep_delay))

```


- Sorting flights to find the flights that left the earliest.
- The flight that left the earliest was B6 97 traveling from JFK to DEN on Dec 07, 2013
- The flight departed 43 minutes ahead of scheduled time

```{r, message = FALSE, warning = FALSE}

# arrange dep_delay column in ascending order

arrange(flights, dep_delay)

```
#### 3.

- Sorting flights to find the fastest (highest speed) flights.
- speed = distance / time
- fastest flights were determined by dividing distance/air_time and then arranging in descending order
- Fastest flight was DL 1499 travelling from LGA to ATL on May 25, 2013

```{r, message = FALSE, warning = FALSE}

arrange(flights, desc(distance/air_time))

```
#### 4.

- Flight that traveled the farthest is HA 51 travelling form JFK to HNL which traveled 4983 miles


```{r, message = FALSE, warning = FALSE}

arrange(flights, desc(distance))


```
- Flight that travelled the shortest is US 1632 travelling form EWR to LGA which travelled 17 miles


```{r, message = FALSE, warning = FALSE}

arrange(flights, distance)


```


## 5.4.1

#### 1. 

- Brainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.

- A) By specifying the four column names

```{r, message = FALSE, warning = FALSE}

select (flights, dep_time, dep_delay, arr_time, arr_delay)

```
- B) By specifying the respective column numbers of the four varialbes

```{r, message = FALSE, warning = FALSE}

select (flights, 4, 6, 7, 9)

```


- C) By specifying the start_with

```{r, message = FALSE, warning = FALSE}

select (flights, starts_with("dep"),starts_with("arr"))

```

- C) By specifying variables within all_of

```{r, message = FALSE, warning = FALSE}

select (flights, all_of(c("dep_time", "dep_delay", "arr_time", "arr_delay")))

```
- D) By specifying variables within any_of

```{r, message = FALSE, warning = FALSE}

select (flights, any_of(c("dep_time", "dep_delay", "arr_time", "arr_delay")))

```

#### 2. 

- What happens if you include the name of a variable multiple times in a select() call?

- The variable is only displayed once. The duplicate is essentially ignored by the select() function 



```{r, message = FALSE, warning = FALSE}

select (flights, dep_time, dep_time, dep_time)

```

#### 3. 

- What does the any_of() function do? Why might it be helpful in conjunction with this vector?

- The any_of() function displays any of the variables, specified within the function

- Using it in conjunction with the vector that's been defined as vars below allows you to define the variables to filter out in one vector, prior to using the any_of() function within select()

- This helps simplify the code written within select ()

- Further, if you are looking to find any of the same variables, within multiple tables, it allows for cleaner duplication of the code (without having to list the variables multiple times) 

#### 4.

- Does the result of running the following code surprise you? How do the select helpers deal with case by default? How can you change that default?

- It is surprising that the specified function ignores the case, by default

```{r, message = FALSE, warning = FALSE}

select(flights, contains("TIME"))

```

- Adding the following argument however changes the default to NOT ignore case

```{r, message = FALSE, warning = FALSE}

select(flights, contains("TIME", ignore.case = FALSE))

```

## 5.5.2

#### 1. 

- Computing dep_time and sched_dep_time to a more convenient representation of number of minutes since midnight


```{r, message = FALSE, warning = FALSE}


# dep_time

transmute(flights,
          dep_time,
  dep_time_hour = dep_time %/% 100,
  dep_time_minute = dep_time %% 100,
  
)

# sched_dep_time

transmute(flights,
          sched_dep_time,
  sched_dep_time_hour = dep_time %/% 100,
  sched_dep_time_minute = dep_time %% 100,
  
)

```

#### 2. 

- Compare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?

- We would expect the computed value to be the same as the air_time value however values within arr_time - dep_time are > than air_time

- This is because the values within arr_time and dep_time are displays of times an not in a format to run the calculation of the difference in times. 

- The first step that needs to be completed is to convert arr_time and dep_time to minutes

- However completing this step and calculating the difference between arr_time and dep_time, still does not provide values that are equivalent to those in the air_time column. In fact, some values are negative

- This is likely due to changes in time zone not being accounted for in our calculation. All arrival and departure times will first need to be converted to the same time zone, prior to completing the calculation for the values in both columns to be the same.


```{r, message = FALSE, warning = FALSE}

transmute(flights, 
       air_time,
       air_time_calculated = arr_time - dep_time,
       )

# convert arrival time and departure time to hours and minutes
# this conversion converts midnight (2400) to 1440
# %% 1440 will convert these values to 0

transmute(flights, 
       air_time,
       arr_time_mins = (arr_time %/% 100 *60 + arr_time %% 100 *60) %% 1440,
       dep_time_mins = (dep_time %/% 100 *60 + dep_time %% 100 *60) %% 1440,
       arr_dep_diff =  arr_time_mins - dep_time_mins
       )
```

#### 3. 

- Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?

- We would expect dep_delay = dep_time - sched_dep_time

- However running the calculation below, we observe that this is note the case.

- As described above, this is likely due to the values within dep_time and sched_dep_time not being listed in minutes. These conversions would first need to be completed in order to complete this calculation

```{r, message = FALSE, warning = FALSE}


transmute (flights, 
        dep_time, 
        sched_dep_time, 
        dep_delay,
        dep_diff = dep_time - sched_dep_time,
        difference = dep_delay -dep_diff
        )

```

#### 4

- Find the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank().

- The min_rank() function assigns the same rank, when the values are tied, which makes sense for the type of ranking that we are trying to complete for the purposes of this exercise

- Further, there are no tied values in the top 10 flights that were delayed

```{r, message = FALSE, warning = FALSE}

# create a data frame with delay rank included

 flights1 <- mutate(flights,
          delay_rank = min_rank(dep_delay)
          )

flights1

# arrange dataframe in descending order of delay rank
# use head function to extract first 10 rows that represent the top 10 most delayed flights

head (arrange(flights1, desc(delay_rank)), 10)


```

#### 5

- What does 1:3 + 1:10 return? Why?
- This code returns the following result:
- 2  4  6  5  7  9  8 10 12 11
- Which is essentially calculated as follows:
- 1 + 1, 2 + 2, 3 + 3, 1 + 4, 2 + 5, 3 + 6, 1 + 7, 2 + 8, 3 + 9, 1 + 10
- since the first vector is smaller than the second, for the, R starts again from the first value of the shorter vector

```{r, message = FALSE, warning = FALSE}

1:3 + 1:10


```
#### 6
- What trigonometric functions does R provide?
- All trigonometric functions provided by R can be found under the help documentation for Trig

- R provides the following functions:

  - sine, cosine, tangent coded as sin(x), cos(x), tan(x), respectively
  - cospi(x) = cos(pi * x), sinpi(x) = sin(pi * x), tanpi(x) = tan(pi *x)
  - arc-cosine, arc-sine and arc-tangent coded as acos(x), asin(x),atan(x), 

- And finally, atan2(y, x), which provides the angle between the x-axis and the vector from (0,0) to (x, y). 

```{r, message = FALSE, warning = FALSE}

?Trig

```

## 5.6

#### 1.

- Brainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. Consider the following scenarios:

- A flight is 15 minutes early 50% of the time, and 15 minutes late 50% of the time.

- A flight is 15 minutes early 50% of the time, and 15 minutes late 50% of the time.

- A flight is always 10 minutes late.

- A flight is 30 minutes early 50% of the time, and 30 minutes late 50% of the time.

- 99% of the time a flight is on time. 1% of the time it’s 2 hours late.

- Which is more important: arrival delay or departure delay?



- Ultimately what is most important to consider is what the organizational problem / decision that the data helps inform is.

- Assessing the delay characteristics in the options mentioned above allows for trying to find answers to the root cause of these delays in multiple ways

- If the analysis is used to help understand how to improve airport processes to minimize delays within the departure airport, then departure delay may be more important. 

- Alternatively if the data is used to help understand  airport processes within the arrival airport could be improved, arrival delay would be more important. 

- Further, if the data is used to understand overall costs associated with delays, then both variables may be eqully important to understand in depth.

- Thus, the research question at hand / the organizational problem in question will help determine how best to group data to inform decisions and which variables to focus on

#### 2.

- Come up with another approach that will give you the same output as not_cancelled %>% count(dest) and not_cancelled %>% count(tailnum, wt = distance) (without using count()).


```{r}

not_cancelled <- flights %>% 
  filter(!is.na(dep_delay), !is.na(arr_delay))

not_cancelled %>%
  count(dest)

# a total count of flights that were not cancelled by destination is provided

# alternative method to generate this using group_by

not_cancelled %>%
  group_by(dest)%>%
  summarise (
    total_not_cancelled = n()
                     )


not_cancelled %>% count(tailnum, wt = distance)

# alternative without using count

not_cancelled %>%
  group_by(tailnum)%>%
  summarise (
    total = sum(distance)
  )



```

#### 3

- Our definition of cancelled flights (is.na(dep_delay) | is.na(arr_delay) ) is slightly suboptimal. Why? Which is the most important column?

- all arr_delay rows = NA however all dep_delay rows are not NA
- That is, there are rows in which a value is logged for dep_delay yet may be missing for arr_del
- This may be for flights that perhaps were delayed in taking off and then had to reroute and did not make it to the final destination
- Thus the most important column is the arr_delay column


```{r}

flights %>% 
  filter(is.na(dep_delay) | is.na(arr_delay)) %>%
  select (dep_delay, arr_delay)%>%
  filter (!is.na(arr_delay))

# all arr_delay rows = NA however all dep_delay rows are not NA


```

#### 4. Look at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the average delay?

There does not appear to be a recurring or cyclical pattern of cancellations within the month but, per the bar chart, certain days like the 8th day of the month showed substantially more cancellations than other days in 2013. Looking at the proportion of cancelled flights and average delays, there does appear to be a relationship. Specifically, as the average arrival delay increases, the proportion of daily cancellations increases. This relationship appears to be exponential. A simple bivariate correlation shows that this relationship is strong and positive (r = 0.59). 

```{r}

# create summary columns for cancellation counts per day and proportions 
flights_cancelled <-

  flights %>%
  mutate(cancelled = (is.na(arr_delay) | is.na(dep_delay))) %>%
  mutate(not_cancelled = (!is.na(arr_delay) | !is.na(dep_delay))) %>%
  group_by(year,month,day)%>%
  summarise(
    cancelled_num = sum(cancelled),
    not_cancelled_num = sum(not_cancelled), 
    prop_cancelled = (cancelled_num /(cancelled_num + not_cancelled_num)),
    delay = mean (arr_delay, na.rm = TRUE)
  )


# show the total number of cancellations per day (totaled over all months)
flights_cancelled %>% 
   group_by(day) %>%
  mutate(cancel_total_per_day = sum(cancelled_num)) %>%
  select(day, cancel_total_per_day) %>%
  distinct(day, .keep_all = TRUE)  %>%
  ggplot(aes(day, cancel_total_per_day)) + geom_bar(stat = "identity") + ggtitle("Total Number of Cancellations per Day of Month") 

# scatter plot shows relationship between avg delay and proportion cancelled 
# relationship looks exponential 
flights_cancelled %>% 
  ggplot(aes(delay, prop_cancelled)) + geom_point() + geom_smooth() 

# simple bivariate correlation shows strong association between avg delay and proportion cancelled 
cor(flights_cancelled$delay, y = flights_cancelled$prop_cancelled) 


```


#### 5 

Which carrier has the worst delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights %>% group_by(carrier, dest) %>% summarise(n()))

F9, EV, and YV had the highest average delays. US and HA had the lowest average delays in 2013. 

```{r}

flights %>%
  filter(!is.na(dep_delay)) %>% 
  group_by(carrier) %>%
  summarise(carrier_delay = mean(dep_delay)) %>% 
  arrange(desc(carrier_delay)) 


flights %>%
  group_by(carrier) %>% 
  filter(!is.na(dep_delay)) %>% 
  summarize(avg_delay = mean(dep_delay)) 



```


#### 6 
What does the sort argument to count() do. When might you use it?

The sort argument of the `count()` function allows the user to show the largest groups that were counted at the top of the data frame that is returned. This would be a good argument to use when wanting to print counts in descending order, similar to `arrange(desc(x))`. The code below uses the sort argument in `count()` to show which airlines logged the most flights in the data set. 

```{r}

flights %>%
  count(carrier, sort = TRUE) 
```

## 5.7.1

#### 1.

- Refer back to the lists of useful mutate and filtering functions. Describe how each operation changes when you combine it with grouping.

Using `group_by` prior to filtering or mutating means that filters or calculations made will be done to within each level of the grouping variable. As a result, in the example below, when we grouped by month, and then calcualted the average delay, it did so for each month. Mutate returned this number as a new column on the data set. The within month calculations can be seen by the repeating numbers for each row. The repeated numbers are expected since we wanted to take the mean within the month. 

```{r}

flights %>%
  group_by(month) %>% 
  filter(!is.na(dep_delay)) %>% 
  mutate(dep_delay_mean = mean(dep_delay)) %>% 
  select(month, dep_delay_mean) %>% head(10) 

```



#### 2.

- Which plane (tailnum) has the worst on-time record?
- Defining worst on-record time as flight with highest mean arrival delay time
- N844MH is the plane with the worst on-time record

```{r}

flights %>%
  filter(!is.na(tailnum), !is.na(arr_time)) %>%
  group_by(tailnum) %>%
  summarise(mean_delay = mean(arr_delay, n=n())) %>%
  arrange (desc(mean_delay))
  
```


#### 3.

- What time of day should you fly if you want to avoid delays as much as possible?
- Morning flights at 7 am tend to be the best time to avoid delays as much as possible

```{r}

flights %>%
  group_by(hour) %>%
  summarise(mean_delay = mean(arr_delay, n=n())) %>%
  arrange((mean_delay))
  
```
#### 4.

- For each destination, compute the total minutes of delay. 

```{r}

flights %>%
  filter(arr_delay >0) %>%
  group_by(dest) %>%
  summarise(total_delay = sum(arr_delay, n=n())) %>%
  arrange((total_delay))
  
```

- For each flight, compute the proportion of the total delay for its destination.

```{r}

flights %>%
  filter(arr_delay >0) %>%
  group_by(dest) %>%
  mutate(
    total_delay = sum(arr_delay),
    prop_delay = arr_delay / total_delay)%>%
  arrange((total_delay))
  
```

#### 5.

- Delays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. 
- Using lag(), explore how the delay of a flight is related to the delay of the immediately preceding flight.

``` {r}

delay_lags <- flights %>%
  group_by(origin) %>%
  mutate(dep_delay_lag = lag(dep_delay)) %>%
  filter(!is.na(dep_delay), !is.na(dep_delay_lag))


ggplot(data = delay_lags, mapping = aes(x = dep_delay_lag, y = dep_delay)) +
          geom_point () +
  scale_x_continuous(breaks = seq(0, 1500, by = 100)) +
  labs(y = "Flight Departure Delay", x = "Previous Flight Departure Delay")
  



```
#### 6
- Look at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error). 

Flight DL 1499 had a speed of 703 miles per hour. That is substantially faster than most of the other recorded speeds. It is possible that distance for this flight was entered as kilometers and not miles. 

- Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?

```{r}

# fast flights
flights %>%
  mutate(hour = air_time/60, speed = distance/hour) %>% 
  arrange(desc(speed)) %>% select(flight, speed) %>% head(10) 

fast_flights <- flights %>%
  filter(!is.na(air_time))%>%
  group_by(dest) %>%
  mutate(
    shortest_air_time = min(air_time),
    relative_air_time = air_time / shortest_air_time
  )%>%
    arrange (desc(relative_air_time))

fast_flights



```

#### 7. Find all destinations that are flown by at least two carriers. Use that information to rank the carriers. 

```{r}

flights %>% 
  group_by(dest, carrier) %>%
  select(dest, carrier) %>%
  ungroup() %>% 
  distinct(dest, carrier) %>% # unique destination and carrier combo 
  count(dest) %>% arrange(n) # show destinations with only one carrier 


# same tidy chunk as above but now filtering out destination not = 1 carrier 
  
multiple_carriers <- flights %>% 
  group_by(dest, carrier) %>%
  select(dest, carrier) %>%
  ungroup() %>% 
  distinct(dest, carrier) %>% 
  count(dest) %>% filter(!n == 1) 

multiple_carriers %>% 
  arrange(desc(n)) # shows destinations with greatest number of carriers 

```


#### 8. For each plane, count the number of flights before the first delay of greater than 1 hour.

```{r}

flights %>%
  select(tailnum, year, month,day, dep_delay) %>%
  filter(!is.na(dep_delay)) %>%
  arrange(tailnum, year, month, day) %>%
  group_by(tailnum) %>%
  mutate(cumulative_min = cumsum(dep_delay)) %>% # using a cumulative function to sequentially add the dep_delay minutes 
  filter(cumulative_min < 60) %>%
  count(tailnum) %>% arrange(desc(n)) %>% head(15) # finally show flight number with total number of fligths 'n' prior to its first 60 min delay. Using a header of full data to limit output for report. 

  
```

# Bulut & Dejardins (2019) Chapter 4 
### Wrangling big data with data.table

This chapter demonstrates the application of the `data.table` in R as an alternative to tidyverse and base R methods of data wrangling. The authors suggest that  `data.table` is particularly useful for working with large volumes of data (e.g. 10 - 100 GB)

```{r, message = FALSE}
library(data.table) 
```

##  4.2.1 Exercises
#### 1. Read in the pisa data set. Either the full data set (recommended to have > 8 Gb of RAM) or one of the smaller data sets.

Due to slow loading speeds when importing the full pisa data set, even with `fread()` from data.table, we decided to use the smaller 'random6' data set that the authors also made a available. According to the authors, the random data set contains data collected from a random sample of a single country with records from all 6 of its regions. 


```{r}
# read in the random data set 
random6 <- fread("random6.csv", na.strings = "") 

# check object size - 0.2 gigabytes 
print(object.size(random6), unit = "GB") 

# read in pisa data set 
#pisa <- fread("pisa2015.csv") 

```

## 4.3.1 Exercises
#### 1. Subset all the Female students (ST004D01T) in Germany. 
#### 2. How many female students are there in Germany?
#### 3. The .N function returns the length of a vector/number of rows. Use chaining with the .N function to answer Exercise 2.

Using data.table syntax to subset the data set, we found that there are 3,197 female students from Germany in the data set. In addition, an alternative method using the ` .N ` function to return the number of rows is shown below as well. 

```{r}
# explore data; first 5 and last 5 rows returned when printing a data.table object 
#random6 # don't run for now due to loading speed 

# subset all female students from the field ST004D01T 

#random6[CNT == "Germany" & ST004D01T == "Female"]

# return row count with .N function and chaining 
#random6[CNT == "Germany" & ST004D01T == "Female", .N] # don't print code as the output is very large in the markdown report 

```



## 4.4.1 Exercises
#### 1. The computer and software variables that were created above ask a student whether they had a computer in their home that they can use for school work (computer) and whether they had educational software in their home (software). Find the proportion of students in the Germany and Uruguay that have a computer in their home or have educational software.

```{r}

# first create custom function to convert bin to numeric  

bin.to.num <- function(x){
  if (is.na(x)) NA
  else if (x == "Yes") 1L
  else if (x == "No") 0L
}


# using the custom function, create new variables, computer and software, that we can use to calculate the proportion of students with access to these resources  


random6[, `:=` 
     (female = ifelse(ST004D01T == "Female", 1, 0),
       sex = ST004D01T,
       
       # At my house we have ...
       desk = sapply(ST011Q01TA, bin.to.num),
       own.room = sapply(ST011Q02TA, bin.to.num),
       quiet.study = sapply(ST011Q03TA, bin.to.num),
       computer = sapply(ST011Q04TA, bin.to.num),
       software = sapply(ST011Q05TA, bin.to.num),
       internet = sapply(ST011Q06TA, bin.to.num),
       lit = sapply(ST011Q07TA, bin.to.num),
       poetry = sapply(ST011Q08TA, bin.to.num),
       art = sapply(ST011Q09TA, bin.to.num),
       book.sch = sapply(ST011Q10TA, bin.to.num),
       tech.book = sapply(ST011Q11TA, bin.to.num),
       dict = sapply(ST011Q12TA, bin.to.num),
       art.book = sapply(ST011Q16NA, bin.to.num))]



```

In Germany, 95% of students in Germany have a computer in their home and 45% of students have educational software. 
```{r}
random6[CNTRYID == "Germany", table(computer)] # 247 No's and 5438 Yes' 
# computer prop 
5438 /(5438 + 247)*100 #  95% of students in data set in Germany have a computer in

random6[CNTRYID == "Germany", table(software)] # 3038 No's and 2481 Yes' 
# educational software prop 
2481/(2481 + 3038) # 45% have access to educational software 

```

In Uruguay, 88% of students have a computer in the home and 43% of students have educational software. 
```{r}
random6[CNTRYID == "Uruguay", table(computer)] # 635 No's and 5099 Yes' 
# computer in home prop 
5099/(5099 + 635) # 88% have access to educational software 


random6[CNTRYID == "Uruguay", table(software)] # 3013 No's and 2313 Yes' 
# educational software prop 
2313/(2313 + 3013) # 43% have access to educational software 
```



#### 2. For just female students, find the proportion of students who have their own room (own.room) or a quiet place to study (quiet.study).

Among female students in the random6 data set, 72% stated that they have their own room and 85% stated that they have a quiet place to study. 
```{r}

random6[female == 1, table(own.room)] # 4805 No's and 12644 Yes' 
# prop own.room

12644 / (12644 + 4805) #72% 

random6[female == 1, table(quiet.study)] #2606 No's and 14801 Yes' 
# prop quiet.study 

14801 / (14801 + 2606) # 85% 

```


## 4.5.1 Exercises
#### 1. Calculate the proportion of students who have art in their home (art) and the average age (AGE) of the students by gender.

51% of students have art in their home. The average age of male and female students are 15.8 years. 

```{r}
# proportion of students who have art in their homes: 
random6[, .(art = mean(art, na.rm = TRUE), AGE = mean(AGE, na.rm = TRUE)), by = .(sex)] 

```


#### 2. Within a by argument you can discretize a variable to create a grouping variable. Perform a median split for age within the by argument and assess whether there are age difference associated with having your own room (own.room) or a desk (desk).

```{r}

# create custom function to split a column of data by above or below median 
median_cut = function(x) { 
  ifelse(x > median(x), "above_median", "below_median")
}


random6[,
     .(own.room = mean(own.room, na.rm = TRUE), 
       desk_mean = mean(desk, na.rm = TRUE)), 
     by = .(median_cut(AGE)) ] 


```

The authors show that the code below, using the `melt()` function, can be used to reshape the data sets from wide to long formats. 
```{r, include = FALSE}
#
random6$id <- 1:nrow(random6) 
athome <- subset(random6, select = c(id, desk:art.book))


# reshape data to long format using melt function 
athome.l <- melt(athome, 
                 id.vars = "id",
                 measure.vars = c("desk", "own.room", "quiet.study", "lit",
                                  "poetry", "art", "book.sch", "tech.book",
                                  "dict", "art.book"))
# view at home long format 
athome.l


```


## 4.8 Lab 

#### 1. This afternoon when we discuss supervised learning, we’ll ask you to develop some models to predict the response to the question Do you expect your child will go into a ?" (PA032Q03TA). Recode this variable so that a “Yes” is 1 and a “No” is a -1 and save the variable as sci_car.

```{r}

# recode and store as 'sci_car'; use table to confirm re-coding
random6[, 
        "sci_car" := sapply(PA032Q03TA, 
                            function(x) { 
                              if (is.na(x)) NA
                              else if (x == "No") -1L 
                              else if (x == "Yes") 1L 
                              
                              }) ][,
                                    table(sci_car)]

```



#### 2. Calculate descriptives for this variable by sex and country. Specifically, the proportion of test takers whose parents said “Yes” or 1.

Because we are using the more limited random6 data set, we specify to return Germany and Mexico specifically, since the other countries are not in the data set. 


```{r}

random6[CNTRYID %in% c("Germany", "Mexico"), 
        .(mean(sci_car, na.rm = TRUE)), 
        by = .(sex, CNTRYID)]

```

#### 3. Means and standard deviations (sd) for the variables that you think will be most predictive of sci_car.

```{r}

# means and sd of variables hypothesized to predict sci_car
random6[, 
        .(environ_avg =  mean(ENVAWARE, na.rm = TRUE), 
          environ_sd = sd(ENVAWARE, na.rm = TRUE), 
          joy_avg = mean(JOYSCIE, na.rm = TRUE), 
          joy_sd = sd(JOYSCIE, na.rm = TRUE), 
          self_eff_avg = mean(SCIEEFF, na.rm = TRUE), 
          self_eff_sd = sd(SCIEEFF, na.rm = TRUE)
          )]


```

#### 4. Calculate these same descriptives by groups (by sci_car and by sex).

```{r}
# means and sd of focal variables, grouped by sci_car and sex 
random6[, 
        .(environ_avg =  mean(ENVAWARE, na.rm = TRUE), 
          environ_sd = sd(ENVAWARE, na.rm = TRUE), 
          joy_avg = mean(JOYSCIE, na.rm = TRUE), 
          joy_sd = sd(JOYSCIE, na.rm = TRUE), 
          self_eff_avg = mean(SCIEEFF, na.rm = TRUE), 
          self_eff_sd = sd(SCIEEFF, na.rm = TRUE)
          ), by = .(sci_car, sex)]

```

#### 5. Calculate correlations between these variables and sci_car


```{r}

# first store smaller data set containing observations for all focal variables and sci_car 
sci_car_variables <- random6[, 
        .(sci_car, ENVAWARE, JOYSCIE, SCIEEFF)]

sci_car_variables[, 
                   .(env_joy_cor = cor(x = ENVAWARE, y = JOYSCIE, use = "complete.obs"),
                   joy_self_eff_cor = cor(x = JOYSCIE, y = SCIEEFF, use = "complete.obs"), 
                   env_sci_car_cor = cor(x = ENVAWARE, y = sci_car, use = "complete.obs"), 
                   sci_car_joy_cor = cor(x = JOYSCIE, y = sci_car, use = "complete.obs"))]



```

#### 6. Create new variables: Discretize the math and reading variables using the OECD means (490 for math and 493) and code them as 1 (at or above the mean) and -1 (below the mean), but do in the data.table way without using the $ operator.

```{r,warning=FALSE}

# recode and store as 'sci_car'; use table to confirm re-coding
random6[, 
        "math_split" := sapply(PV1MATH, 
                            function(x) { 
                              if (is.na(x)) NA
                              else if (x <= 490) -1L 
                              else if (x > 490) 1L 
                              
                              })][,
                                    table(math_split)]

random6[, 
               "reading_split" := sapply(PV1READ, 
                            function(x) { 
                              if (is.na(x)) NA
                              else if (x <= 493) -1L 
                              else if (x > 493) 1L}) ][, 
                                                     table(reading_split)]


# calculate correlations with these new variables and those above 
# store as table for subsequent formatting 
math_reading_cor <- random6[, 
                   .(env_joy_cor = cor(x = ENVAWARE, y = JOYSCIE, use = "complete.obs"),
                   joy_self_eff_cor = cor(x = JOYSCIE, y = SCIEEFF, use = "complete.obs"), 
                   env_sci_car_cor = cor(x = ENVAWARE, y = sci_car, use = "complete.obs"), 
                   sci_car_joy_cor = cor(x = JOYSCIE, y = sci_car, use = "complete.obs"), 
                   math_joy_cor = cor(x = math_split, y = sci_car, use = "complete.obs"), 
                   reading_joy_cor = cor(x = reading_split, y = sci_car, use = "complete.obs"), 
                   math_env_cor = cor(x = ENVAWARE, y = math_split, use = "complete.obs"), 
                   reading_env_cor = cor(x = JOYSCIE, y = reading_split, use = "complete.obs"), 
                   math_self_eff_cor = cor(x = SCIEEFF, y = math_split, use = "complete.obs"), 
                  reading_self_eff_cor = cor(x = SCIEEFF, y = reading_split, use = "complete.obs"))] 

# pivot long using melt(); easier to view correlations this way 
melt(math_reading_cor) 

```

#### 7. Chain together a set of operations: For example, create an intermediate variable that is the average of JOYSCIE and INTBRSCI, and then calculate the mean by country by sci_car through chaining.

```{r}

random6[, .(Mean = rowMeans(.SD)), by = CNTRYID, .SDcols = c("JOYSCIE", "INTBRSCI")][,,by = sci_car]

```



#### 8. Transform variables, specifically recode MISCED and FISCED from characters to numeric variables.

```{r}

# create column subset to convert to numeric
cols_to_convert <- c("MISCED", "FISCED") 

# convert to numeric with subset index and using lapply() 
random6[ , 
           (cols_to_convert) := lapply(.SD, as.numeric),
           .SDcols = c("MISCED", "FISCED")]


```

#### 9. Examine other variables in the pisa data set that you think might be predictive of PA032Q03TA.

```{r}
library(psych) 

random6[, lapply(.SD, mean), .SDcols = c("DISCLISCI", "TEACHSUP", "IBTEACH", "TDTEACH")]


# run descriptive statistics for three additional variables hypothesized to relate to PA032Q03TA 
descriptive_stats <- random6[ ,lapply(.SD, psych::describe), 
         .SDcols = c("DISCLISCI", "TEACHSUP","TDTEACH")] 

# reshape table into long format so that it is easier to read the output
melt(descriptive_stats) 




```




