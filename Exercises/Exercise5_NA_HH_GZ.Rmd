---
title: "Exercise5_NA_HJ_GZ"
author: "Gian Zlupko"
date: "2022-12-305"
output: html_document
---



#### Libraries used through exercise 
```{r, message = FALSE}
library(topicmodels)
library(tm)
library(tidyverse)
library(tidytext) 
library(tidyr)
library(slam)
library(ggrepel) 
library(MASS)
library(textstem)
library(readtext)
```


# Part I: Recreate CTM Markdown Analysis 

The following code in Part I of this exercise submission recreates code provided by Professor Bowers used to fit correlated topic models (CTM) and to use CTM to summarize large text data. The text data used in this assignment are positive movie reviews posted on International Move Data Base (IMDB)'s online movie review platform. 


To get started, first we read in text data 

```{r}
# Read .txt files directly from a folder
# Set a directory first
data_dir <- "/Users/gianzlupko/Desktop/ORLA 6541 Data Science/ORLA_6541_Applied_Data_Science/imdb_pos_reviews"

# Read in .txt data using
# We need the encoding line to scrub weird non-standard characters out of the text data
data_big <- readtext(paste0(data_dir, "/*.txt"), encoding = "UTF-8")
# check dimensions 
dim(data_big)
```


Next, we collect a sample 100 rows from the full data set of postive reviews. 

```{r}
# collect a random sample and look at a data preview 
sample_reviews <- sample_n(data_big, 100) 
head(sample_reviews) 


# export the subset of the larget data set to local and share with team members 
library(xlsx) 
write.csv(sample_reviews, "subset_imdbd_pos_reviews.csv") 
write.xlsx(sample_reviews, "subset_imdbd_pos_reviews.xlsx") 
```


[optional] Here we can read in the subset of the larger data set if we want to instead of running the import code from the raw text data each time. 

```{r}
```



#### Document-term matrix (DTM)

After loading in the data, we need to create a special data structure that topic models use to perform their clustering-like, and dimensionality reduction-like, methods of the data. Specifically, topic models need a DTM. DTMs are matrices that contain word frequency counts for documents for all words in the corpus' overall vocabulary. 

To create one, the `Corpus()` function is used from the `tm` package. 

```{r}
# corpus
corpus <- Corpus(VectorSource(sample_reviews$text))
corpus

# build DTM 
text_DTM <- DocumentTermMatrix(corpus,control = list(stemming=TRUE, stopwords = TRUE, minWordLength = 3, removeNumbers = TRUE, removePunctuation = TRUE))

# view characteristics of the DTM created 
text_DTM

# dimensions 
dim(text_DTM)

```

The dimensions of the DTM [100,3741] show that there are the 100 rows that were randomly selected and 3,741 columns. Those columns are the terms (e.g. words). 


Next we use term frequency-inverse document frequent (TF-IDF) to 

This measure allows to omit terms which have low frequency as well as those occurring in many documents (Hornik & Grun, 2011) 
```{r}
term_tfidf <- tapply(text_DTM$v / row_sums(text_DTM)[text_DTM$i], text_DTM$j, mean) * log2(nDocs(text_DTM)/col_sums(text_DTM > 0))
summary(term_tfidf)
```

Plot TF-IDF 

```{r}
plot(density(term_tfidf))
```


ggplot2 version of the same plot above: 

```{r}
# uses data from above, creates a df, and pipes directly into ggplot2 
data.frame(term_tfidf) %>% 
  mutate(doc_id = seq(length(term_tfidf))) %>% 
  rename(tf_idf = term_tfidf) %>% 
  ggplot(aes(x = tf_idf)) + geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) + ylab("Density") + xlab("TF-IDF")
  

```







Use median tf-idf to select terms to retain for topic modeling. Per the output above, the median was 0.04. 
```{r}
# use median
alpha <- 0.04
text_DTM_trimmed <- text_DTM[row_sums(text_DTM) > 0, term_tfidf >= alpha]
dim(text_DTM_trimmed)
```



#### 10-fold cross validation 

```{r}
# Cross validation

control_CTM_VEM <- list(
  estimate.beta = TRUE, verbose = 0, prefix = tempfile(), save = 0, keep = 0,
  seed = as.integer(Sys.time()), nstart=1L, best = TRUE,
  var = list(iter.max=100, tol=10^-6),
  em = list(iter.max=500, tol=10^-4),
  cg = list(iter.max=100, tol=10^5)
)

# use 10-fold CV to determine k!
# randomly divide the data into 10 folds.

set.seed(100)
topics <- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15) ##set k equals to 2 3 4 5 6 7 8 9 10 15.
seed <- 2
D <- length(sample_reviews$text) 
folding <- sample(rep(seq_len(10), ceiling(D))[seq_len(D)])
table(folding)
```

We train and test using the 10-fold cross validation that we just set up. Then plot by perlexity to see where the elbow is to select the correct number of topics k. This step can take a little while to run. The code in the for loop below is generating a correlated topic model (CTM) for the vector of k values that we created above (see the object, 'topics'). Then, for each CTM that was created, the for loop is extracting the perplexity statistic from the models. Later, we will plot the perplexity to see which value of k is optimal given the data. In generally, generating topic models requires run time as they use iterative resampling methods to fit whichever k-topic solution is requested to the data. Thus, the code chunk below will take time to run as it is generating 10 total CTMs. 


```{r}
## write a loop to automatically output the perplexity
perp_by_col <- vector()
for (k in topics) {
  perp_by_row <- vector()
  for (chain in seq_len(10)) {
    training <- CTM(text_DTM_trimmed[folding != chain,], k = k,
                    control = control_CTM_VEM)
    testing <- CTM(text_DTM_trimmed[folding == chain,], model = training,
                   control = control_CTM_VEM)
    perp_by_row <- rbind(perp_by_row, perplexity(testing))
  }
  perp_by_col <- cbind(perp_by_col, perp_by_row)
}
```


Plot perplexity following 10-fold cross validation 

Perplexity is a loglikelihood measure of topic model performance. It indicates the extent to which the topic model was able to predict data that was left out. Another way to understand left-out likelihood is a measure of how well the topic model reproduces the characteristics of data that was left out. 


```{r}
# Plot perplexity
transpose <- t(perp_by_col)
matplot(transpose, type = "l", col = rainbow(9), lty = 2, lwd = 2, ylab = "Perplexity", xlab = "K", main = "CTM-10-fold cross validation", xaxt="n")
axis(1, at=1:10, labels = c("k=2", "k=3", "k=4", "k=5", "k=6", "k=7", "k=8", "k=9", "k=10", "k=15"), cex=0.5)

perp_by_col_mean <- colMeans(perp_by_col)

lines(perp_by_col_mean, col = "black", lwd = 4, lty = 1)
led <- c("fold=2", "fold=3", "fold=4", "fold=5", "fold=6", "fold=7", "fold=8", "fold=9", "fold=10", "Average")
legend("topright", led, lwd = 2, lty = 2, col = c(rainbow(9), 'black'), cex = 0.65)

abline(v = 4, col = "gray60", lty = 2)
```


Plot average perplexity 

```{r}
# Average Perplexity
{plot(perp_by_col_mean, pch = 20, ylab = 'Perplexity', xlab = "K", main = "CTM-10-fold cross validation", 
      xaxt = "n") 
  axis(1, at = 1:10, labels = c("k=2","k=3","k=4","k=5","k=6","k=7","k=8","k=9","k=10","k=15"), cex = 0.5)
  lines(perp_by_col_mean, lwd = 1, lty = 2, col = "red")}
```


The perplexity plots indicate that perplexity is best for k = 9 topics. Thus, for the remainder of this exercise, we have retained the CTM with k = 9 topics. 

#### CTM Model Selection 

```{r}
control_CTM_VEM1 <- list(
  estimate.beta = TRUE, verbose=0, prefix=tempfile(),save=0,keep=0,
  seed=1421313709,nstart=1L,best=TRUE,
  var=list(iter.max=500,tol=10^-6),
  em=list(iter.max=1000,tol=10^-4),
  cg=list(iter.max=500,tol=10^5)
)
control_CTM_VEM

# below we generate a 9-topic CTM 

CTM9 <- CTM(text_DTM_trimmed, k = 9, control = control_CTM_VEM1, 
            seed = 12244) # set seed for reproducibility 
CTM9
```


#### CTM Output 

```{r}
## A CTM_VEM topic model with 9 topics.

## Topics
topics9 <- posterior(CTM9)$topics
## Let's look at the probability of each document info fits into each of the topics
topics3 <- as.data.frame(topics9)
rownames(topics9) <- sample_reviews$name
#print(topics9)

```


```{r}
## Let's look at which topic each document is assigned to one of the topics.
main_topic9 <- as.data.frame(topics(CTM9))
rownames(main_topic9) <- sample_reviews$doc_id
colnames(main_topic9) <- "Main_Topic"
print(main_topic9)
```

#### Top terms by topic 

Now, we can visualize the terms (words) that are 

```{r}
# Using this: https://www.tidytextmining.com/topicmodeling.html
# Use tidyverse to look at the CTM results a bit more more

tidy_topics <- tidy(CTM9, matrix = "beta")
tidy_topics

# create a top terms df; filtering by top 10 terms per topic 
top_terms <- tidy_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)
top_terms

```



Visualize the top 10 terms for each topic. 

```{r}
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```


#### Multidimensional scaling (MDS)

MDS can be used to operationalize 'distance' between topics. This step is one way to provide an data-driven indication as to how similar any two topics may be to one another. 


```{r}
# Classical MDS
# N rows (objects) x p columns (variables)
# each row identified by a unique row name

d <- dist(topics9) # euclidean distances between the rows

fit <- isoMDS(d, k=2) # k is the number of dim

# If there are identical rows. Run the below only if there are identical rows

#library(vegan)
#fit <- vegan::metaMDS(comm = dist(topics))

fit # view results
```


```{r}
# plot solution 
# add the main topic as column 3

plot_data <- as.data.frame(cbind(fit$points[,1], fit$points[,2], main_topic9$Main_Topic), 
                                 row.names = sample_reviews$doc_id)
colnames(plot_data) <- c("Coordinate1", "Coordinate2", "Main_Topic")


(p1 <- ggplot(data = plot_data, aes(x = Coordinate1, y = Coordinate2)) + geom_point(size=2, shape=23)) 
```

```{r}
(p2 <- p1 + geom_point() + geom_text_repel(aes(label = row.names(plot_data)), size = 3, max.overlaps = 20)) 
```




```{r}
# Need to use as.factor for Main_Topic for aes color so that there is a discrete color palette

(p4 <- ggplot(data = plot_data) +
    geom_point(mapping = aes(x = Coordinate1, y = Coordinate2, color = as.factor(Main_Topic))))
```




```{r}
(p5 <- ggplot(plot_data, aes(Coordinate1, Coordinate2, color = as.factor(Main_Topic)))+
    geom_point()+geom_text_repel(aes(label = row.names(plot_data)), size = 3, max.overlaps = 30))
```





# Part II: Additional Data Visualizations 

#### Instructions for Part II: 
*After replicating the analysis in the example markdown in #1 above, please extend the analysis using additional code, visualizations, or inclusion of additional data from the IMDB movie review data set. Please include at least three additional extensions that you include at the end of the markdown for Exercise 5. Please include at least one of the following ideas as one of your three, the other two are up to you or feel free to stick to this list, itâ€™s up to your team:* 

*Display the full text of each of the top 3 highest probability documents for each identified topic (using code, not copy paste from the .txt).*

* Extension options to choose from (select at least one)
   * Display full text for top 3 highest probability documents for each topic 
   * Develop a visualization from the Silge & Robinson (2017) tidytext reading
   * Examine how sentiment relates to the topics 
   * Using the data and links to the movies that the movie reviews in the IMDB 
   * Other text mining examples from google scholar or elsewhere using the IMDB ddta set 

### Extension 1 


Relationship between sentiment and topics

```{r}
library(textfeatures) 


# assign topic from CTM to observations 
# use the function topics() from topicmodels library to assign the most
# likely topics for each document (in this case combined reasons) 

topicmodels::topics(CTM9) 
topic_assigned <- as.data.frame(topicmodels::topics(CTM9)) 
topic_assigned$row_id <- rownames(topic_assigned) 
colnames(topic_assigned) <- c("topic_assigned", "row_id") 
topic_assigned

# add the same row_id column to the original data: 
sample_clean <- data.frame(sample_reviews) 
sample_clean$row_id <- rownames(sample_clean) 

# perform data join to attach topic proportions to original data
sample_clean <- sample_clean %>% 
  left_join(topic_assigned, by = "row_id")

head(sample_clean) 

# now we'll look at sentiment in a tidy text format following 
# Silge and Robinson (2017) 

sample_clean %>%
  filter(topic_assigned == 7) %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  inner_join(get_sentiments("bing")) %>%
  mutate(sent_score = ifelse(sentiment == "negative", 1, -1)) %>% 
  group_by(row_id) %>% 
  summarize(sent_count = sum(sent_score)) %>% 
  ggplot(aes(x = row_id, y = sent_count)) + geom_histogram(stat = 'identity') + 
  labs(x = "Review ID", y = "Sentiment Total", title = "Sentiment Distribution for Movie Reviews Categorized by Topic 7")


```


### Extension 2 

visualize word differences from the Tidytext book 
```{r}

library(tidyr)

# using CTM model object, extract beta values and store in a matrix 
review_topics <- tidy(CTM9, matrix = "beta")

# pivot from long to a wide data format and 
# calculate a ratio of topic 2: topic 1 likelihood (uses beta) 
beta_wide <- review_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

# create the plot with a random selection of 15 rows 
beta_wide %>% 
  sample_n(size = 15) %>% 
  ggplot(aes(x = reorder(term, desc(log_ratio)), y = log_ratio)) + geom_histogram(stat = 'identity') + coord_flip() + ylab("Log Ratio of beta in Topic 2 / Topic 1") + xlab("Term") 



```






### Extension 3 

Visualizing word frequencies 


```{r}
# start with raw data again

sample_clean %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  count(word) %>% 
  arrange(desc(n)) %>%
  mutate(freq = (n/3892)) %>%
  ggplot(aes(x = freq, y  = freq)) + geom_point(position = "jitter", alpha = .25) + 
  scale_x_log10() + scale_y_log10() 

word_freq

ggplot(data = word_freq, aes(x = freq))


review_topics %>% 
  group_by(term) %>%
  count(term) %>% 
  arrange(desc(n))  
```






# Part III: 
