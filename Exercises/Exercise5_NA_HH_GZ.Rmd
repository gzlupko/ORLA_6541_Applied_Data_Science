---
title: "Exercise5_NA_HJ_GZ"
author: "Gian Zlupko"
date: "2022-12-305"
output: html_document
---



#### Libraries used through exercise 
```{r, message = FALSE}
library(topicmodels)
library(tm)
library(tidyverse)
library(tidytext) 
library(tidyr)
library(slam)
library(ggrepel) 
library(MASS)
library(textstem)
library(readtext)
```


# Part I: Recreate CTM Markdown Analysis 

The following code in Part I of this exercise submission recreates code provided by Professor Bowers used to fit correlated topic models (CTM) and to use CTM to summarize large text data. The text data used in this assignment are positive movie reviews posted on International Move Data Base (IMDB)'s online movie review platform. 


To get started, first we read in text data 

```{r}
# Read .txt files directly from a folder
# Set a directory first
data_dir <- "/Users/gianzlupko/Desktop/ORLA 6541 Data Science/ORLA_6541_Applied_Data_Science/imdb_pos_reviews"

# Read in .txt data using
# We need the encoding line to scrub weird non-standard characters out of the text data
data_big <- readtext(paste0(data_dir, "/*.txt"), encoding = "UTF-8")
# check dimensions 
dim(data_big)
```


Next, we collect a sample 100 rows from the full data set of postive reviews. 

```{r}
# collect a random sample and look at a data preview 
sample_reviews <- sample_n(data_big, 100) 
head(sample_reviews) 


# export the subset of the larget data set to local and share with team members 
library(xlsx) 
write.csv(sample_reviews, "subset_imdbd_pos_reviews.csv") 
write.xlsx(sample_reviews, "subset_imdbd_pos_reviews.xlsx") 
```




#### Document-term matrix (DTM)

After loading in the data, we need to create a special data structure that topic models use to perform their clustering-like, and dimensionality reduction-like, methods of the data. Specifically, topic models need a DTM. DTMs are matrices that contain word frequency counts for documents for all words in the corpus' overall vocabulary. 

To create one, the `Corpus()` function is used from the `tm` package. 

```{r}
# corpus
corpus <- Corpus(VectorSource(sample_reviews$text))
corpus

# build DTM 
text_DTM <- DocumentTermMatrix(corpus,control = list(stemming=TRUE, stopwords = TRUE, minWordLength = 3, removeNumbers = TRUE, removePunctuation = TRUE))

# view characteristics of the DTM created 
text_DTM

# dimensions 
dim(text_DTM)

```

The dimensions of the DTM [100,3741] show that there are the 100 rows that were randomly selected and 3,741 columns. Those columns are the terms (e.g. words). 


Next we use term frequency-inverse document frequent (TF-IDF) to 

This measure allows to omit terms which have low frequency as well as those occurring in many documents (Hornik & Grun, 2011) 
```{r}
term_tfidf <- tapply(text_DTM$v / row_sums(text_DTM)[text_DTM$i], text_DTM$j, mean) * log2(nDocs(text_DTM)/col_sums(text_DTM > 0))
summary(term_tfidf)
```

Plot TF-IDF 

```{r}
plot(density(term_tfidf))
```


ggplot2 version of the same plot above: 

```{r}
# uses data from above, creates a df, and pipes directly into ggplot2 
data.frame(term_tfidf) %>% 
  mutate(doc_id = seq(length(term_tfidf))) %>% 
  rename(tf_idf = term_tfidf) %>% 
  ggplot(aes(x = tf_idf)) + geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) + ylab("Density") + xlab("TF-IDF")
  

```







Use median tf-idf to select terms to retain for topic modeling. Per the output above, the median was 0.04. 
```{r}
# use median
alpha <- 0.04
text_DTM_trimmed <- text_DTM[row_sums(text_DTM) > 0, term_tfidf >= alpha]
dim(text_DTM_trimmed)
```



#### 10-fold cross validation 

```{r}
# Cross validation

control_CTM_VEM <- list(
  estimate.beta = TRUE, verbose = 0, prefix = tempfile(), save = 0, keep = 0,
  seed = as.integer(Sys.time()), nstart=1L, best = TRUE,
  var = list(iter.max=100, tol=10^-6),
  em = list(iter.max=500, tol=10^-4),
  cg = list(iter.max=100, tol=10^5)
)

# use 10-fold CV to determine k!
# randomly divide the data into 10 folds.

set.seed(100)
topics <- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15) ##set k equals to 2 3 4 5 6 7 8 9 10 15.
seed <- 2
D <- length(sample_reviews$text) 
folding <- sample(rep(seq_len(10), ceiling(D))[seq_len(D)])
table(folding)
```

We train and test using the 10-fold cross validation that we just set up. Then plot by perlexity to see where the elbow is to select the correct number of topics k. This step can take a little while to run. The code in the for loop below is generating a correlated topic model (CTM) for the vector of k values that we created above (see the object, 'topics'). Then, for each CTM that was created, the for loop is extracting the perplexity statistic from the models. Later, we will plot the perplexity to see which value of k is optimal given the data. In generally, generating topic models requires run time as they use iterative resampling methods to fit whichever k-topic solution is requested to the data. Thus, the code chunk below will take time to run as it is generating 10 total CTMs. 


```{r}
## write a loop to automatically output the perplexity
perp_by_col <- vector()
for (k in topics) {
  perp_by_row <- vector()
  for (chain in seq_len(10)) {
    training <- CTM(text_DTM_trimmed[folding != chain,], k = k,
                    control = control_CTM_VEM)
    testing <- CTM(text_DTM_trimmed[folding == chain,], model = training,
                   control = control_CTM_VEM)
    perp_by_row <- rbind(perp_by_row, perplexity(testing))
  }
  perp_by_col <- cbind(perp_by_col, perp_by_row)
}
```


Plot perplexity following 10-fold cross validation 

Perplexity is a loglikelihood measure of topic model performance. It indicates the extent to which the topic model was able to predict data that was left out. Another way to understand left-out likelihood is a measure of how well the topic model reproduces the characteristics of data that was left out. 


```{r}
# Plot perplexity
transpose <- t(perp_by_col)
matplot(transpose, type = "l", col = rainbow(9), lty = 2, lwd = 2, ylab = "Perplexity", xlab = "K", main = "CTM-10-fold cross validation", xaxt="n")
axis(1, at=1:10, labels = c("k=2", "k=3", "k=4", "k=5", "k=6", "k=7", "k=8", "k=9", "k=10", "k=15"), cex=0.5)

perp_by_col_mean <- colMeans(perp_by_col)

lines(perp_by_col_mean, col = "black", lwd = 4, lty = 1)
led <- c("fold=2", "fold=3", "fold=4", "fold=5", "fold=6", "fold=7", "fold=8", "fold=9", "fold=10", "Average")
legend("topright", led, lwd = 2, lty = 2, col = c(rainbow(9), 'black'), cex = 0.65)

abline(v = 4, col = "gray60", lty = 2)
```


Plot average perplexity 

```{r}
# Average Perplexity
{plot(perp_by_col_mean, pch = 20, ylab = 'Perplexity', xlab = "K", main = "CTM-10-fold cross validation", 
      xaxt = "n") 
  axis(1, at = 1:10, labels = c("k=2","k=3","k=4","k=5","k=6","k=7","k=8","k=9","k=10","k=15"), cex = 0.5)
  lines(perp_by_col_mean, lwd = 1, lty = 2, col = "red")}
```


The perplexity plots indicate that perplexity is best for k = 9 topics. Thus, for the remainder of this exercise, we have retained the CTM with k = 9 topics. 

#### CTM Model Selection 

```{r}
control_CTM_VEM1 <- list(
  estimate.beta = TRUE, verbose=0, prefix=tempfile(),save=0,keep=0,
  seed=1421313709,nstart=1L,best=TRUE,
  var=list(iter.max=500,tol=10^-6),
  em=list(iter.max=1000,tol=10^-4),
  cg=list(iter.max=500,tol=10^5)
)
control_CTM_VEM

# below we generate a 9-topic CTM 

CTM9 <- CTM(text_DTM_trimmed, k = 9, control = control_CTM_VEM1, 
            seed = 12244) # set seed for reproducibility 
CTM9
```


#### CTM Output 

```{r}
## A CTM_VEM topic model with 9 topics.

## Topics
topics9 <- posterior(CTM9)$topics
## Let's look at the probability of each document info fits into each of the topics
topics3 <- as.data.frame(topics9)
rownames(topics9) <- sample_reviews$name
#print(topics9)

```


```{r}
## Let's look at which topic each document is assigned to one of the topics.
main_topic9 <- as.data.frame(topics(CTM9))
rownames(main_topic9) <- sample_reviews$doc_id
colnames(main_topic9) <- "Main_Topic"
print(main_topic9)
```

#### Top terms by topic 

Now, we can visualize the terms (words) that are 

```{r}
# Using this: https://www.tidytextmining.com/topicmodeling.html
# Use tidyverse to look at the CTM results a bit more more

tidy_topics <- tidy(CTM9, matrix = "beta")
tidy_topics

# create a top terms df; filtering by top 10 terms per topic 
top_terms <- tidy_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)
top_terms

```



Visualize the top 10 terms for each topic. 

```{r}
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```


#### Multidimensional scaling (MDS)

MDS can be used to operationalize 'distance' between topics. This step is one way to provide an data-driven indication as to how similar any two topics may be to one another. 


```{r}
# Classical MDS
# N rows (objects) x p columns (variables)
# each row identified by a unique row name

d <- dist(topics9) # euclidean distances between the rows

fit <- isoMDS(d, k=2) # k is the number of dim

# If there are identical rows. Run the below only if there are identical rows

#library(vegan)
#fit <- vegan::metaMDS(comm = dist(topics))

fit # view results
```


```{r}
# plot solution 
# add the main topic as column 3

plot_data <- as.data.frame(cbind(fit$points[,1], fit$points[,2], main_topic9$Main_Topic), 
                                 row.names = sample_reviews$doc_id)
colnames(plot_data) <- c("Coordinate1", "Coordinate2", "Main_Topic")


(p1 <- ggplot(data = plot_data, aes(x = Coordinate1, y = Coordinate2)) + geom_point(size=2, shape=23)) 
```

```{r}
(p2 <- p1 + geom_point() + geom_text_repel(aes(label = row.names(plot_data)), size = 3, max.overlaps = 20)) 
```




```{r}
# Need to use as.factor for Main_Topic for aes color so that there is a discrete color palette

ggplot(data = plot_data) +
    geom_point(mapping = aes(x = Coordinate1, y = Coordinate2, color = as.factor(Main_Topic))) + theme(legend.position = "none") 
```




```{r}
(p5 <- ggplot(plot_data, aes(Coordinate1, Coordinate2, color = as.factor(Main_Topic)))+
    geom_point()+geom_text_repel(aes(label = row.names(plot_data)), size = 3, max.overlaps = 30))
```





# Part II: Additional Data Visualizations 

#### Instructions for Part II: 
*After replicating the analysis in the example markdown in #1 above, please extend the analysis using additional code, visualizations, or inclusion of additional data from the IMDB movie review data set. Please include at least three additional extensions that you include at the end of the markdown for Exercise 5. Please include at least one of the following ideas as one of your three, the other two are up to you or feel free to stick to this list, it’s up to your team:* 

*Display the full text of each of the top 3 highest probability documents for each identified topic (using code, not copy paste from the .txt).*

* Extension options to choose from (select at least one)
   * Display full text for top 3 highest probability documents for each topic 
   * Develop a visualization from the Silge & Robinson (2017) tidytext reading
   * Examine how sentiment relates to the topics 
   * Using the data and links to the movies that the movie reviews in the IMDB 
   * Other text mining examples from google scholar or elsewhere using the IMDB ddta set 

### Extension 1 


Relationship between sentiment and topics

```{r}
library(textfeatures) 


# assign topic from CTM to observations 
# use the function topics() from topicmodels library to assign the most
# likely topics for each document (in this case combined reasons) 

topicmodels::topics(CTM9) 
topic_assigned <- as.data.frame(topicmodels::topics(CTM9)) 
topic_assigned$row_id <- rownames(topic_assigned) 
colnames(topic_assigned) <- c("topic_assigned", "row_id") 
topic_assigned

# add the same row_id column to the original data: 
sample_clean <- data.frame(sample_reviews) 
sample_clean$row_id <- rownames(sample_clean) 

# perform data join to attach topic proportions to original data
sample_clean <- sample_clean %>% 
  left_join(topic_assigned, by = "row_id")

head(sample_clean) 

# now we'll look at sentiment in a tidy text format following 
# Silge and Robinson (2017) 

sample_clean %>%
  filter(topic_assigned == 7) %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  inner_join(get_sentiments("bing")) %>%
  mutate(sent_score = ifelse(sentiment == "negative", 1, -1)) %>% 
  group_by(row_id) %>% 
  summarize(sent_count = sum(sent_score)) %>% 
  ggplot(aes(x = row_id, y = sent_count)) + geom_histogram(stat = 'identity') + 
  labs(x = "Review ID", y = "Sentiment Total", title = "Sentiment Distribution for Movie Reviews Categorized by Topic 7")


```


### Extension 2 

visualize word differences from the Tidytext book 
```{r}

library(tidyr)
library(htmltools)

# using CTM model object, extract beta values and store in a matrix 
review_topics <- tidy(CTM9, matrix = "beta")

# pivot from long to a wide data format and 
# calculate a ratio of topic 2: topic 1 likelihood (uses beta) 
beta_wide <- review_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

# create the plot with a random selection of 15 rows 
beta_wide %>% 
  sample_n(size = 15) %>% 
  ggplot(aes(x = reorder(term, desc(log_ratio)), y = log_ratio, fill = log_ratio >0)) + geom_histogram(stat = 'identity') + coord_flip() + ylab("Log Ratio of beta in Topic 2 / Topic 1") + xlab("Term") + scale_fill_manual(values = c("red", "blue")) + theme(legend.position = "none")

```






### Extension 3 

Visualizing word frequencies 


```{r}
# generate bigrams with each term of bi-gram in separate column 
library(tidyr) 

bigrams <- sample_clean %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word1 == "br") %>% 
  filter(!word2 == "br") %>%
  unite(bigram, word1, word2, sep = " ")

# visualize the most common bigrams
bigrams %>% 
  count(bigram) %>%
  arrange(desc(n)) %>% 
  head(10) %>%
  ggplot(aes(x = reorder(bigram, n), y = n, fill = bigram)) + geom_histogram(stat = "identity") + coord_flip() + theme(legend.position = "none") + xlab("Bigram") + ylab("Count") + ggtitle("Most Common Bigrams in Sample Reviews")


```

The sample histogram output is useful in visualizing the most common n-grams in the data set. However, additional visualizations can lend deeper insights into word co-occurances within the data set. For example, the following visualization presents a network of word co-occurances. The network is based on words that co-occur with one another. By using a network visualization, we can see which words were written, not only in bi-term patterns, but in denser network clusters. 




```{r}

bigram_counts <- sample_clean %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE) 

# bi-gram network visualization 
library(igraph) 
library(ggraph) 
bigram_graph <- bigram_counts %>% 
  filter(n > 2) %>%
  graph_from_data_frame()

set.seed(2017)
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

```

This is a small data set so there aren't too many clusters of term correlations that contain three or more terms but we can see a few network clusters that emerged from the visualization above. One cluster of common word co-occurances contains the terms, 'films', 'horror', 'budget', and 'low'. This cluster of terms is likely being discussed in reviews that are about low budget horror films. Another cluster of terms contains the words, 'park', 'wook', and 'chan'. These terms were likely stated in movie reviews about Park Chan-Wook, a South Korean movie director. In a larger data set with more data, a network approach to n-gram visualization would likely provide larger network communities. 


# Part III: 

#### Provide a 2-3 page single spaced brief research proposal in which you argue for and justify the use of Correlated Topic Models applied to a research topic that you are interested in. Please address the following questions in this order as you apply your new knowledge of these techniques (feel free to write this part in MS Word or similar and copy/paste into the markdown). 

*a. What is the purpose of your study?*

The purpose of the study is to apply Correlated Topic Models to analyze written reviews regarding the "Big Three" consulting firms (McKinsey & Co., The Boston Consulting Company, and Bain & Co) to examine which topics are associated with higher or lower overall ratings, as described by employees. 

*b. Is there any research literature and theory that supports this argument? How so? *

  Examining the overall rating given to given to these firms can serve as a proxy for employee satisfaction. Roznowski and Hulin (1992, p. 26) described job satisfaction as "the most informative data a manager or researcher can have for predicting employee behavior." Additionally, Lambert et al. concluded that job satisfaction is a highly prominent predictor of turnover intentions (2001).     Additionally, analyzing Glassdoor's "Top Review Highlights by Sentiment" for these companies identified culture, salary, and flexibility as common pros, whereas work-life balance, managers, and late hours are recurrent cons. Past literature suggests that culture, specifically Organizational Citizenship Behaviors (OCB), which measures the extent to which workers report willingness to help their coworkers (Lambert, 2000), is associated with lower intentions to quit. Likewise, control over work schedule, which could serve as a proxy for flexibility, was found to be related to lower odds of reporting an intent to quit (Kennedy and Mohr, 2022), and salary has been historically positively associated with job satisfaction. 						On the other hand, poor work-life balance and long hours can induce work to family conflict, which is positively related to burnout (Pleck et al. 1980) and absenteeism (Goff et al. 2006) while negatively associated with job satisfaction (Bedeian et al. 1988; Bacharach et al. 1991). In addition, poor management is related to job satisfaction. For example, Family-Supportive Supervisor Behaviors, which evaluates supervisors' support for integrating work and family, as perceived by employees (Hammer et al., 2009), is significantly related to family-work conflict (Han & Mclean, 2020) and was found to have a positive effect on reducing employees' intentions to quit (Virdo & Daly, 2019).
Despite the useful information already provided by Glassdoor, we hope that applying Correlated Topic Model will reveal more topics and allow for further analysis to examine their relationship with overall rating. 


*c. Why is a Correlated Topic Model a means to address this purpose? *

  Correlated Topic Model identifies latent topics as features across the employee reviews. These topics can then be used as independent variables in subsequent models evaluating their relationship to the overall rating. 


*d. What would be the research question(s)? (To what extent…) *

  How many distinct topics exist across employee reviews of the "big three" consulting firms, what are they, and do they differ between the firms? To what extent are these topics associated with overall rating, controlling for other variables such as location?


*f. What types of data would you be looking for? *

I will be looking for unique terms across the reviews which repeat themselves frequently enough to be classified into topics, excluding high and extremely low-frequency words.

*g. Provide the generalized equation for the topic model and a brief narrative in which you specify the type of model, following the examples from the readings. *

Topic modeling takes an unsupervised approach to uncover latent topics in documents. Unlike LDA, correlated topic modeling uses the multivariate normal distribution instead of the Dirichlet with the goal of generating k values. To do so, it needs k means and k standard deviations. To transform the multivariate normal distribution into probabilities, CTM passes the values through a variant of the logistic function. 

```{r}
# Load libraries needed
library(png)
library(magick)

# Read in image
img <- image_read("/Users/gianzlupko/Desktop/ORLA 6541 Data Science/ORLA_6541_Applied_Data_Science/ctm_formula.png")

# Visualize the image
plot(img)
```
In addition to the CTM model formula, a plate diagram for CTM is shown below. 

```{r}
# Read in image
img1 <- image_read("/Users/gianzlupko/Desktop/ORLA 6541 Data Science/ORLA_6541_Applied_Data_Science/ctm_platediagram.png")

# Visualize the image
plot(img1)
```

Where η ('eta') is a vector k of values of arbitrary sizes, ηⱼ is every output multivariate normal, and j corresponds to a topic.


*h. What do you think you would find? *

I believe that work-life balance, managers, hours, culture, salary, and flexibility will be the prevailing latent topics in employee reviews. In addition, I hope that further analysis will reveal these topics' relationship with the overall rating. For example, while culture and managers might prevail as review topics, they might be associated with both positive and negative reviews. 

*i. Why would this be important? What would be the implications for this research domain?*
	
	Due to the demanding nature of working for the "big three" consulting firms, it is important to understand employees' perspectives and the underlying factors affecting their job satisfaction. Taking an unsupervised approach to identify these factors and analyzing their relationship to employees' objective rating of their firm could provide these organizations with directions to increase their employees' satisfaction. 
	





# Appendix 

### A1. R solutions for common NLP tasks 


N-grams 

```{r}

# create column with n-grams 
sample_clean %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) 

# generate bigrams with each term of bi-gram in separate column 
library(tidyr) 

bigrams <- sample_clean %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  unite(bigram, word1, word2, sep = " ") %>%
  filter(!bigram == "br br")


bigrams %>% 
  count(bigram) %>%
  arrange(desc(n)) %>% 
  head(10) %>%
  ggplot(aes(x = reorder(bigram, n), y = n, fill = bigram)) + geom_histogram(stat = "identity") + coord_flip() + theme(legend.position = "none") + xlab("Bigram") + ylab("Count") 
  

  
```


#### A2: Stop Words 


Stop words using `tidytext` package 
```{r}

library(tidytext) 
# remove stop word dictionary from tidytext package 
sample_clean %>%
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)


# remove custom list of stop words 

# create custom vector of words that you don't want
words_to_remove <- c("movie", "amc", "picture", "amc", "hollywood", "Sandra")

# remove custom list from words 
sample_clean %>% 
  unnest_tokens(word, text) %>% 
  filter(!word %in% words_to_remove)

sample_clean %>% 
  anti_join()


str <- c("I have zero a accordance")

custom_stopwords = c("a", "able", "about", "above", "abst", "accordance", "yourself",
"yourselves", "you've", "z", "zero")


df_stopwords <-
  tibble(
    word = c("em", "de", "o", "para")
  )

sample_clean %>% 
  anti_join(df_stopwords)  



```



#### A3: Create DTM in tidy format 

```{r}


test_1 <- sample_clean %>% 
  rename(document = doc_id) 

sample_dtm <- test_1  %>%
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>%
  count(document, word, sort = T) %>%
  cast_dtm(document = document, term = word, n) 

# sample LDA using the DTM 
test_CTM <- CTM(sample_dtm, k = 5, control = list(seed = 1234))
test_CTM 

```









