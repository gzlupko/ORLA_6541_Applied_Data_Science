---
title: "Exercise 3 - HCA & Heatmaps"
author: "Nilima Ajaikumar, Hagay Jalon, and Gian Zlupko"
date: '2022-10-24'
output: html_document
---


# Exercise 3
## Hierarchical Cluster Analysis and Heatmaps

Libraries used throughout exercise 
```{r, warning = FALSE, message = FALSE }
library(ggplot2)
library(tidyverse) 
library(broom)
library(data.table) 
library(ComplexHeatmap)
library(circlize)
library(hopach)
```


## Part I: 
#### Please replicate the code and markdown provided in the readings (Bowers) for the mtcars dataset to visualize it using hierarchical cluster analysis heatmaps following the recommendations from the readings, including using uncentered correlation.


#### For this project, we will first use the first seven continuous variables in the dataset, and reserve variable 8 and 9 for dichotomous (0/1) annotations in a future step.
```{r}
data<-mtcars[, c(1:7)]
```

#### Rescale the dataset
```{r}
# In cluster analysis, we want to z-score the data, so that all continuous variables are on the same scales, otherwise a variable with bigger numbers will pull the entire data matrix towards it.

data.scale<-scale(data)
head(data.scale)
```

#### Create first Heatmap with ComplexHeatmap

```{r}
Heatmap(data.scale, name = "mtcars heatmap", 
        cluster_rows=FALSE , cluster_columns = FALSE)

# This is an unclustured heatmap. 
# In this heatmap low values are indicated in blue and high values are indicated in red  

```

#### Default Clustering on the rows

```{r}
Heatmap(data.scale, name = "mtcars heatmap", 
        cluster_columns = FALSE)

# In this heatmap we want to know what the patterns are for the rows only
# So we cluster only on the rows

# Our question is to cluster the cars by these seven variables to understand which cars are most similar to each other based on this data.

# The dendrogram (“cluster tree”) on the left shows the similarity of the rows. Longer lines indicate more dissimilarity.

#  We observe two big clusters. At the “top” (Chrysler Imperial to Duster 360) these are the gas guzzlers with low mpg (blue) and the most cylinders (red), many are big (the Imperial and Continental weigh the most), with a lot of horse power, high weight, etc. 

# We also observe quite a bit of variance in qsec and draft as well as across the variables.

# The bottom part are our more fuel efficient cars, but again still quite a bit of variance.

# The defaults provide some useful patterns that are interesting, but we can do better.

```

#### Default cluster on the rows and the columns
``` {r}

Heatmap(data.scale, name = "mtcars heatmap")

# Here we cluster both rows and columns at the same time so that not only are the most similar rows next to each other, but the most similar columns are also next to each other, and we get an indication of similarity with the dendrograms.

# We begin to see the classic “checkerboard” pattern in this heatmap as both the rows and columns are clustered and similar patterns are next to each other in each dimension. The row order from the previous heatmap is retained.

# The top cluster indicates that they all have low qsec (they’re slow), low mpg (gass guzzlers), with low draft (blues). They weigh a lot, and have a lot of horse power, etc (reds). 

```

#### Clustering using uncentered correlation and average linkage.
```{r}
# Declare a function for uncentered distance using cosangle
uncenter.dist<-function(m) {
  as.dist(as.matrix(distancematrix(m, d="cosangle")))
}

# Now we can call uncenter.dist() to cluster our rows and columns using average linkage “ave”.

# Note that for the columns, we need to transpose the matrix, so we  include the t() function.

row.clus<-hclust(uncenter.dist(data.scale), method = "ave")
col.clus<-hclust(uncenter.dist(t(data.scale)), method = "ave")

Heatmap(data.scale, name = "mtcars heatmap", 
        cluster_rows=row.clus, cluster_columns=col.clus)

# In this fully clustered HCA heatmap, the distance metric is Euclidean, and the agglomeration clustering algorithm is uncentered correlation with average linkage.

# Comparing this heatmap with  our previous defaults HCA heatmap. Here, the more robust clustering algorithm gives us a clearer hierarchical structure of the data, with a much clearer pattern.

# Note first that the Ferrari Dino is correctly patterned as the most extreme outlier as it has the longest horizontial line in the row dendrogram, as it is the only car with high horsepower, but low weight etc, and is fast.

# The “fuel efficient” block at the top of the heatmap is now much more obvious as it is blue on the left and mostly red on the right. The gas guzzler muscle cars are now more obvious as well in the lower block as they are red on the left and blue on the right, with high horsepower but high weight, but they are slow to accelerate and have terrible fuel efficiency (low mpg).

```

#### HCA Heatmap Annotations of 0/1 Dichtomous Variables

``` {r}

# We can make this visualization more useful by adding information about 0/1 dichotomous variables.


# Following Bowers(2010) for format, here we will use two 0/1 dichotomous variables, if the car is a manual=1 or automatic=0, and if the engine block layout for the cylinders is inline=1 or in a V pattern=0.

# The mtcars dataset has this information in column 8 and 9. So we extract that information first. 

#Then we will color code the annotations as white=0, black=1. This format aids the eye in seeing the differences. Then we tell ComplexHeatmap how to label the annotations and how wide we want the annotation column (0.5 cm)

ht_inline<-Heatmap(mtcars[, c(8)], name = "Cylinder Config",
                   col = colorRamp2(c(0,1),c("white", "black")),
                   heatmap_legend_param = list(at = c(0,1),
                         labels = c("V-shaped", "Inline")),
                   width = unit(0.5,"cm"))

ht_manual<-Heatmap(mtcars[, c(9)], name = "Transmission",
                   col = colorRamp2(c(0,1),c("white", "black")),
                   heatmap_legend_param = list(at = c(0,1),
                         labels = c("Automatic", "Manual")),
                   width = unit(0.5,"cm"))

# Then, we use Heatmap() as above, but this time assign it to ht_main, then then use the draw() command to put it all together.

#With the annotations, we have not changed the order of the rows or columns, as the annotation data is not part of the clustering analysis. 

#But with this added information, we can now come to more informed conclusions about why the clusters are patterning in the way they are here. 

ht_main = Heatmap(data.scale, name = "cluster rows",
                  cluster_rows=row.clus, cluster_columns = col.clus)

draw(ht_main+ht_manual+ht_inline, auto_adjust = FALSE) 

# We observe  that almost all of the “top” cluster cars, which include our most fuel efficient cars in the 1970s were manual trasmission (stick shift) with inline straight cylinder engines. 

#The gas guzzlers and muscle cars were much more often automatics and V-shaped cylinder engine designs.

```

#### Final HCA heatmap 
```{r}
# The same final HCA heatmap without the row labels. 
draw(ht_main+ht_manual+ht_inline) 

```




## Part II: 

#### Select at least three different options/alternatives from ComplexHeatmap's first three chapters, and apply them to the mtcars dataset. For example, such as using different clustering algorithms (k-means, etc.), separation of heatmaps, stacking, different ways to annotate (columns and rows and different graphing options beyond black/white blocks), etc. Please provide some text in your markdown discussing what options/alternatives you selected, why this was interesting, and any challenges you experienced with running the visualizations. Provide a few sentences to interpret each new heatmap visualization.


mtcars data set 

```{r}
# create new data set 
data_mat <- as.matrix(mtcars) 

# scale data 
data_mat <- scale(data_mat) 
```

### Custom HCA heatmap 1: 

Below we visualize a heatmap using a categorical variable for annotation with more than two levels. 

```{r}
# update row and col clustering with new data set input into the custom functions created 
row.clus<-hclust(uncenter.dist(ht_1_dat), method = "ave")
col.clus<-hclust(uncenter.dist(t(ht_1_dat)), method = "ave")

# remove the cylinder variable from the data matrix; will use cylinders as annotation 
ht_1_dat <- data_mat[, -2]

# set cyl as annotation on heatmap 
ht_cyl <-Heatmap(mtcars[, c(2)], name = "Cyl Count",
                   col = colorRamp2(c(4,6,8),c("light grey", "grey", "black")),
                   heatmap_legend_param = list(at = c(0,1,2),
                         labels = c("4", "6", "8")),
                   width = unit(0.5,"cm")) 

ht_main <- Heatmap(ht_1_dat, name = "cluster rows", 
                   cluster_rows = row.clus, cluster_columns = col.clus)

# put together annotated heatmap using draw() function 
draw(ht_main + ht_cyl, auto_adjust = FALSE) 

```

Heatmap description: For the HCA heatmap above, we used uncentered correlation as our clustering technique and average linkage as our agglomeration method. Our primary focus was to produce a heatmap that would allow us to compare cars with 4, 6, and 8 cylinder engines. To do so, we visualized the cylinder variable from the mtcars data set as an annotation for the heatmap. 


Heatmap interpretation: The cylinder count for car engines can tell us a lot about what to expect from a car. More cylinders often means more horsepower and less MPG. The heatmap confirms these assumptions while giving more patterns to interpret. First, we see that 8 cylinder engines also tend to be heavier than cars with fewer cylinders. In addition, we also see that most of the 8 cylinder cars have automatic transmissions whereas the cars with fewer cylinders have a greater balance of manual and automatic transmissions. Thus, the use of an HCA heatmap was interesting for our interest in cylinder count. It allowed us to confirm some of our assumptions about the general relationships between cylinder count, horsepower, and MPG while also providing a much richer and nuanced assessment of key differences between cars with different cylinder counts. 


Challenges: For the HCA heatmap above, we visualized engine cylinder count as a categorical variable for annotation. We ran into challenges creating an categorical annotation with more than two levels as was shown in the given code in course readings. Specifically, during our early attempts, we received the error: "The length of 'at' should be the same as the length of 'labels'. Initially, this error message appeared somewhat cryptic to us. However, upon looking at the code chunk used to edit the data corresponding to heatmap annotation, we realized that we needed to pass a third value through the 'at' command in the that the length of our variable needed to match the `heatmap_legend_param` argument. After adding a third value, corresponding with the three levels of our categorical variable, we were then able to successfully render the visualization.


### Custom HCA heatmap 2: 

Split by k-means clustering 
Create a 2x2 with row and column partitions applied to a heatmap clustered by k-means. 


```{r}

Heatmap(data_mat, 
        row_km = 2, 
        column_km = 2, 
        row_gap = unit(2, "mm"), 
        column_gap = unit(1, 'mm'), border = TRUE,
        column_title = "2x2 Mtcars Grid with K-means Clustering",
        column_title_gp = gpar(fontsize = 15, fontface = "bold"), row_title = "Car Groups")

```


Heatmap description: The heatmap above uses k-means clustering to produce a 2x2 grid of the cars in the mtcars data set. By arbitrarily specifying k = 2 for the k-means technique, our goal was to generate a grid by which we could visualize simple high and low comparisons of cars across multiple variables. The best way to interpret this heatmap is to read the heatmap horizontally, observing the values of the the variables along the bottom of the heatmap for each car. In general, 2x2 grids are interesting in that they generate simple comparisons across high and low levels of a set of variables. Moreover, they allow for four general classes to draw comparisons from. In the grid above, we can imagine adding names to each quadrant that describe general qualities of the cars. For example, the upper left quadrant could be named 'Big Engines, the upper right quadrant could be named 'Gas Guzzlers', the lower right named 'Fuel Efficient' and the lower left named 'Small Engines'. Also, while 2x2 grids can be overly simplistic, we wanted to leverage k-means for this task was k-means requires a priori specification.


Heatmap Interpretation: We found multiple patterns in the data from this heatmap. First, regarding miles per gallon (MPG), the bottom two quadrants of our heatmap were more fuel efficient than cars in the upper quadrants. Moreover, cars in the bottom quadrants were lighter in weight, had smaller engines, and were slower (note: 'qsec', or 1/4 mile time). By contrast, cars in the upper quadrants had larger engines, more horsepower, were faster, and as a result, were less fuel efficient. 


Challenges: One challenge that we experienced while visualizing the heatmap above was determining the appropriate distances to separate the data into quadrants. For example, in the current arrangement, it is possible that readers will not immediately perceive the continuity between the quadrants that are horizontally placed next to one another. That is, it is possible to misinterpret the fact that, while there is separate between each set of horizontal quadrants, the data therein represent underlying information for the same car across the rows. To improve the legibility of this heatmap while retaining our desire to produce a 2x2 grid, we altered the column and row splits such that the column split was half as large was the row split. In doing so, our hope was to retain the gestalt of a 2x2 grid while minimizing the distance between columns such that the patterns across column may be interpreted as pertaining to the same car. 




### Custom HCA heatmap 3: 
Comparison of U.S. and International car manufacturers. We created a new variable from the car model names to determine whether the car was made by U.S. manufacturers or by international car makers. 

```{r}
library(xlsx) 
#write.xlsx(mtcars,"mtcars.xlsx") 
# read back into R with first column as true to get a character vector; this export-import step allowed us to grab the column for the car names, which we could not index originally. 
mt_full <- read.xlsx("mtcars.xlsx", sheetIndex = 1) 

# rename the car model name column and store as character. Then use an ifelse() statement in a mutate statement to create a new column that provides a label for international cars (1) and american cars (0)
mt_full <- mt_full %>% 
  rename(model = NA.) %>% 
  mutate(model = as.character(model)) %>% 
  mutate(international = ifelse(model == "Cadillac Fleetwood" | model == "Lincoln Continental" | model == "Chrysler Imperial" | model == "Dodge Challenger" | 
                                 model == "Camarro Z28" | model == "Pontiac Firebird" | model == "Ford Pantera L" | model == "AMC Javelin" | model == "Hornet Sportabout" | model == "Hornet 4 Drive" | model == "Duster 360" | model == "Valiant", 0, 1)) 

# remove the international label for now; will use the international categorical variable to annotate the heatmap
mt_model <- mt_full %>% 
  select(-c(international, model)) 

# convert data frame to matrix and scale the matrix 
mt_model <- as.matrix(mt_model) 
mt_model <- scale(mt_model) 

# add histogram boxplot for weight of cars 
wt_mat <- mt_model[,6] # create matrix subset for wt variable 

histo_anno = rowAnnotation(Weight = anno_barplot(mt_wt, gp = gpar(fill = 3))) 

# format the international variable for
ht_intl <-Heatmap(mt_full[, 13], name = "Int'l Model", right_annotation = histo_anno,
                   col = colorRamp2(c(0,1),c("grey", "black")),
                   heatmap_legend_param = list(at = c(0,1),
                         labels = c("No", "Yes")),
                   width = unit(0.5,"cm"), column_title = "U.S. and International Cars")

# now remove wt fromt the data for the heatmap 
mt_model_clean <- mt_model[, -6]

# uncentered correlation on the mt_model data
ht_models <- Heatmap(mt_model_clean, name = "cluster rows", 
                   cluster_rows = row.clus, cluster_columns = col.clus)

# combine the heatmap with the annotation 
ht_list3 <- ht_models + ht_intl
draw(ht_list3, ht_gap = unit(.5, "cm"))

```
 
 Heatmap description: The HCA heatmap above used uncentered correlation as the clustering technique and average linkage as the agglomeration method. To create a U.S. vs. International car comparison, we custom generated a categorical variable by manually specifying American and non-American car makers using the car model names. Initial inspection of previous iterations of the heatmap above revealed that car weighted appeared to be a key difference between America and non-American cars. Therefore, we created a second annotation for weight to accompany the manufacturer location annotation. Specifically, we created a bar plot annotation for the weight variable.
 
 
Heatmap interpretation:  American cars appeared to be heavier than non-American cars. These heavier car weights could be due to a number of factors that also appear in the data, such a larger engines (displacement and cylinders). Car weights may also differ as a function of factors not captured in the data but that nonetheless are interesting to infer given the clusters generated. Specifically, American cars of this period (late '70s) were known for being produced with heavy steel. In contrast, European and Asia car manufacturers may have used different supplies at this time. It would be worth consulting a car expert on such history but nonetheless, the use of an HCA heatmap allows for such an inquiry where other methods may not provide such an efficient gestalt to build  questions from. 

Challenge: The annotation for the weight variable gave us a challenge. It took multiple iterations to achieve our desired bar plot annotation for the weight variable. In fact, we originally started with a histogram annotation but the result was messy. The histogram ranges were too wide, resulting in less of an annotation and more of a second full-size plot alongside the heatmap. In contrast, the bar plot provided a more succinct annotation. 





## Part III: Brief Sample Research Proposal 


### A.	What is the purpose of your study?
The purpose of this study is to apply a hierarchical cluster analysis (HCA) heatmap as a descriptive method to better understand the associations between various psychological factors relating to employee, family, and organizational well-being, to employees' intention to quit. These variables include emotional burnout, family supportive supervisor behavior, family-to-work conflict, job satisfaction, psychological job demands, organizational citizenship behavior, low-value work, decision authority, and control over work hours. 
Subsequently, this study aims to further examine the effects of an organizational intervention designed to increase supervisor social support and control over work schedule on the affiliated associations and intention to quit levels. Lastly, this study hopes to dissect the effects of the intervention based on one's role as an employee or a manager. 

### B.	Is there any research literature and theory that supports this argument? How so? 

The relationship between intention to quit to the following variables and related literature is described below:
*Control over work schedule.* In a recent study, Kennedy and Mohr found that schedule control was independently related to greater odds of reporting an intent to quit (2022). Additionally, control over work schedule was found to moderate the relationships between other psychological variables and intention to quit. For example, in 2018, Lee and Eissenstat found that greater control over work hours was directly associated with reduced work-family conflict, which had a significant relationship with burnout. Given the strong positive correlation between burnout and ITQ, it is reasonable to expect that control over work hours may have a strong negative relationship with ITQ. Furthermore, employees with higher control over their work schedule, combined with high decision authority and high job demands, were found to be less inclined to quit (Berridge et al., 2018; Lee et al., 2020)

#### Job Strain – Decision Authority and Psychological Job Demands 
The interaction between job demands and decision authority was found to predict levels of intention to quit. When job demands are high and decision authority is low, employees' well-being decreases. Contrariwise, when both decision authority and psychological job demands are high, employees' motivation and performance increase (Karasek et al., 1998). These findings suggest that higher decision authority buffers the potential negative impact of high psychological job demands. 

#### Low-Value Work
This study measured low-value work as employees' mean response to two questions on a 1 to 5 Likert Scale: "You work on unnecessary things" and "You spend time in unproductive meetings". While there is a lack of literature regarding the relationship between low-value work, meaningful work was found to increase employees' psychological well-being (Chalofsky, 2003;Hackman and Oldham, 1980). As such, this study expects to find an association between low-value work and intention to quit. 
#### Family-Supportive Supervisor Behaviors.

Family-Supportive Supervisor Behaviors (FSSB) Assesses employee perceptions of supervisors' behavioral support for integrating work and family (Hammer et al., 2009). Virdo and Daly found FSSB to have a positive effect on reducing intent to quit (2019). 

#### Work-Family Conflict

Previous research suggests that Work-Family Conflict is positively correlated with burnout (Pleck et al. 1980), depression (Frone et al. 1992), absenteeism (Goff et al. 2006), and turnover intention (Burke 1988).

#### Organizational Citizenship Behaviors

A cross-cultural study by Coyne and Ong found a significant relationship between OCB turnover intention (2007). Similarly, Ulndag et al. found positive relationships between OCB to job satisfaction, and a negative association between OCB and turnover intentions.

#### Burnout

Perhaps the most familiar predictor of intention to quit. A 2021 random-effects meta-analysis of 3,842 teachersd found a significant positive relationship between burnout and intention to quit, with burnout explaining 63% of the variance in teachers' ITQ (Madigan & Kim, 2021), all else equal.

#### Job Satisfaction 

Rahman et al. (2008) found that job satisfaction negatively affected IT professionals' turnover intentions. Similarly, Lambart et al. concluded that job satisfaction is a highly prominent predictor of turnover intentions. 
C.	Why is cluster analysis heatmaps a means to address this purpose?
A cluster analysis heatmap is suitable as it visualizes the magnitude of the relationships between intention to quit to the various variables discussed. Heatmaps create a juxtaposition of associations, which allows for easy visual comparison between these relationships. 

### D.	What would be the research question(s)? 

1.	*What are the most significant predictors of intention to quit?*
2.	*Was the intervention successful in increasing control over work schedule and supervisor social support? If yes, were these increase associated with reduced intentions to quit?*
3.	*Was the intervention's success dependent on individuals' roles as employees or managers?* 

### E.	What type of dataset would you need? Is there a dataset you know of that would work?
I would need a data set consisting of mostly numerical variables. I will use the Work, Family and Health Network data set, a four-wave survey of two companies conducted between 2009 and 2012, to assess workplace practices' effects on employee, family, and organizational well-being.

### F.	What types of data would you be looking for?
As most items were collected using a Likert-Scale, I will be looking mostly at ordinal data. For cluster analysis purposes, ordinal Likert scale data will be treated as continuous, relaxing assumptions, as is commonly done with common analytic techniques like regression in the social sciences. Moreover, to support the decision to treat Likert data as continuous, scale distributions can be inspected for univariate normality. 


### G.	Provide the generalized equation for the clustering and a brief narrative in which you specify the type of clustering, following the examples from the readings.
Bowers' recommended clustering algorithm for education data, suggests that the distance measure used is uncentered correlation (2010). This would allow us to show both the absolute change in the pattern and the magnitude of the difference. Furthermore, Bowers recommends using average linkage as the agglomeration method because it is robust to missing data. According to Xu's and Wunsch's recommendations, the hierarchical clustering used would cluster individuals based on similarities to form larger clusters (2005). The generalized equation for uncentered correlation is as follows: 

```{r pressure, echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics("uncentered_corr.png")
```




### H.	What do you think you would find?
I believe I would find positive relationships between ITQ and emotional burnout, family-to-work conflict, psychological job demands, and low-value work. While I anticipate ITQ to have a negative relationship with family-supportive supervisor behavior, job satisfaction, organizational citizenship behavior, decision authority, and control over work hours. In addition, I believe that the intervention would successfully reduce ITQ by increasing decision authority and family supportive supervisor behavior, and control over work hours. I anticipate these findings to be more robust among employees compared to managers. 

### I.	Why would this be important? What would be the implications for this research domain?

Employee turnover can have detrimental effects on businesses. Although the factors influencing turnover had been well-studied for decades, their application into workplaces had rarely succeeded, leading to the current "Great Resignation." This study hopes to examine employees' intentions by investigating a large number of predictive factors and revealing clusters of employees who display patterns associated with higher ITQ, shedding light on which employees are predicted to quit. 



## Part IV: Optional Stretch Goal 

Use the TidyTuesday data from September of 2021 on the Billboard Top 100 songs chart from Spotify 

```{r}
# Get the Data
library(tidytuesdayR) 
# This loads the readme and all the datasets for the week of interest

# Either ISO-8601 date or year/week works below 

tuesdata <- tidytuesdayR::tt_load('2021-09-14')
#tuesdata <- tidytuesdayR::tt_load(2021, week = 38)

billboard <- tuesdata$audio_features

# Or read in the data manually

```

Focusing on describing the data by song, select songs based on:
spotify_track_popularity > 90.
Then replicate as many of the aspects of the below cluster analysis heatmap as you can, which may include among other aspects:
•	Row order
•	Column order
•	Annotations
•	Font size
•	Labels
Provide a few sentences within your markdown on the successes and challenges in working to replicate as many aspects of the visualization as you can.

```{r}
head(billboard)
```
```{r}
# filter by popular 
billboard_over_90 <- billboard %>% 
  filter(spotify_track_popularity > 90)

data.scale<-scale(billboard_over_90)
head(data.scale)


# generate initial heatmap 
Heatmap(data.scale, name = "billboard heatmap", 
        cluster_rows=FALSE , cluster_columns = FALSE)
```



```{r}
uncenter.dist<-function(m) {
  as.dist(as.matrix(distancematrix(m, d="cosangle")))
}
```

```{r}
row.clus<-hclust(uncenter.dist(data.scale), method = "ave")
col.clus<-hclust(uncenter.dist(t(data.scale)), method = "ave")

Heatmap(data.scale, name = "billboard heatmap", 
        cluster_rows=row.clus, cluster_columns=col.clus)
```

```{r}
Heatmap(data.scale, name = "billboard heatmap", 
    column_km = 3, column_title_gp = gpar(fill = c("red", "blue", "green"), font = 1:3),
    column_names_gp = gpar(col = c("green", "orange", "purple"), fontsize = c(10, 14, 8)))
```



